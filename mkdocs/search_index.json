{
    "docs": [
        {
            "location": "/", 
            "text": "GRACC\n\n\nGRid ACcounting Collector\n\n\n(pronounced \"grok\")\n\n\n\n\nGRACC is a collection of components for implementing resource usage accounting.\n\n\nWhy GRACC?\n\n\nGRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.\n\n\nRepositories of interest:\n\n\n\n\nGRACC Collector\n.  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.\n\n\nGRACC Monitoring Emails\n.  Simple daily emails overview GRACC activity.\n\n\nGRACC Request Agent\n. Agents that listen for replay requests from GRACC.\n\n\nGRACC Summary Agent\n. Agents that request summary records from the Request Agent and forward the records back to the Logstash agent for storage in ElasticSearch\n\n\nGRACC Archiver\n.  Agents that listen on the RabbitMQ exchange for raw records and store them to disk in a gzip file.\n\n\nGRACC Backup Scripts\n. A service that runs periodically on the same host as the Archiver which backs up the completed gzip files to external Tape.  See \nBackup Docs\n for more details.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#gracc", 
            "text": "GRid ACcounting Collector  (pronounced \"grok\")   GRACC is a collection of components for implementing resource usage accounting.", 
            "title": "GRACC"
        }, 
        {
            "location": "/#why-gracc", 
            "text": "GRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.  Repositories of interest:   GRACC Collector .  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.  GRACC Monitoring Emails .  Simple daily emails overview GRACC activity.  GRACC Request Agent . Agents that listen for replay requests from GRACC.  GRACC Summary Agent . Agents that request summary records from the Request Agent and forward the records back to the Logstash agent for storage in ElasticSearch  GRACC Archiver .  Agents that listen on the RabbitMQ exchange for raw records and store them to disk in a gzip file.  GRACC Backup Scripts . A service that runs periodically on the same host as the Archiver which backs up the completed gzip files to external Tape.  See  Backup Docs  for more details.", 
            "title": "Why GRACC?"
        }, 
        {
            "location": "/user/standard/", 
            "text": "Standard GRACC Interfaces\n\n\nGrafana\n\n\nGrafana\n is an open-source graphing/dashboarding web application,\nwidely used for system and service monitoring. The GRACC project maintains a Grafana \ndeployment with a collection of standard dashboards at \n\nhttps://gracc.opensciencegrid.org\n.\n\n\nKibana\n\n\nWhile Grafana is great for looking at pre-defined views into the data, it's not \ngreat for exploration or ad-hoc analytics, which is the area \nKibana\n excels in. The GRACC project maintains a public read-only\nKibana interface at \n\nhttps://gracc.opensciencegrid.org/kibana\n.", 
            "title": "Standard Interfaces"
        }, 
        {
            "location": "/user/standard/#standard-gracc-interfaces", 
            "text": "", 
            "title": "Standard GRACC Interfaces"
        }, 
        {
            "location": "/user/standard/#grafana", 
            "text": "Grafana  is an open-source graphing/dashboarding web application,\nwidely used for system and service monitoring. The GRACC project maintains a Grafana \ndeployment with a collection of standard dashboards at  https://gracc.opensciencegrid.org .", 
            "title": "Grafana"
        }, 
        {
            "location": "/user/standard/#kibana", 
            "text": "While Grafana is great for looking at pre-defined views into the data, it's not \ngreat for exploration or ad-hoc analytics, which is the area  Kibana  excels in. The GRACC project maintains a public read-only\nKibana interface at  https://gracc.opensciencegrid.org/kibana .", 
            "title": "Kibana"
        }, 
        {
            "location": "/user/grafana/", 
            "text": "Accessing GRACC from Grafana\n\n\nExisting Grafana\n\n\nIf you already have a Grafana instance that you maintain, it's very easy to\nadd a GRACC datasource so you can start incorporating accounting data into\nyour dashboards.\n\n\nDatasource Settings\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\n\n\n\n\n\n\n\n\nType\n\n\nElasticsearch\n\n\n\n\n\n\nUrl\n\n\nhttps://gracc.opensciencegrid.org/q\n\n\n\n\n\n\nAccess\n\n\nProxy (either should work)\n\n\n\n\n\n\nIndex name\n\n\ngracc.osg.summary\n\n\n\n\n\n\nPattern\n\n\nno pattern\n\n\n\n\n\n\nTime field name\n\n\n@timestamp\n\n\n\n\n\n\nVersion\n\n\n5.x\n\n\n\n\n\n\nGroup by time interval\n\n\n1d", 
            "title": "Custom Grafana"
        }, 
        {
            "location": "/user/grafana/#accessing-gracc-from-grafana", 
            "text": "", 
            "title": "Accessing GRACC from Grafana"
        }, 
        {
            "location": "/user/grafana/#existing-grafana", 
            "text": "If you already have a Grafana instance that you maintain, it's very easy to\nadd a GRACC datasource so you can start incorporating accounting data into\nyour dashboards.", 
            "title": "Existing Grafana"
        }, 
        {
            "location": "/user/grafana/#datasource-settings", 
            "text": "Setting  Values      Type  Elasticsearch    Url  https://gracc.opensciencegrid.org/q    Access  Proxy (either should work)    Index name  gracc.osg.summary    Pattern  no pattern    Time field name  @timestamp    Version  5.x    Group by time interval  1d", 
            "title": "Datasource Settings"
        }, 
        {
            "location": "/user/direct/", 
            "text": "Direct Access\n\n\nGRACC runs on \nElasticsearch\n,\nwhich can be accessed via many \nclient libraries\n and tools. \n\n\nA read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q.\n\n\ncURL\n\n\nThe Elasticsearch \nquery DSL\n is quite complex, but is also very powerful.\nBelow is a relatively simple but non-trivial example of directly querying GRACC from the \ncommand-line via cURL. This query will calculate the number of payload jobs run by \nLIGO in January 2017, and the total wall time they used. \n\n\ncurl 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary '\n{\n    \nquery\n: {\n        \nquery_string\n: {\n            \nquery\n: \nVOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01]\n\n        }\n    },\n    \naggs\n: {\n        \nwalltime\n: {\n            \nsum\n: {\n                \nfield\n: \nCoreHours\n\n            }\n        },\n        \njobs\n: {\n            \nsum\n: {\n                \nfield\n: \nNjobs\n\n            }\n        }\n    },\n    \nsize\n: 0\n}'\n\n\n\n\nWhich might return:\n\n\n{\n  \ntook\n : 2,\n  \ntimed_out\n : false,\n  \n_shards\n : {\n    \ntotal\n : 6,\n    \nsuccessful\n : 6,\n    \nfailed\n : 0\n  },\n  \nhits\n : {\n    \ntotal\n : 68,\n    \nmax_score\n : 0.0,\n    \nhits\n : [ ]\n  },\n  \naggregations\n : {\n    \njobs\n : {\n      \nvalue\n : 20626.0\n    },\n    \nwalltime\n : {\n      \nvalue\n : 41323.43916666666\n    }\n  }\n}\n\n\n\n\nCompare to the \nVO Summary\n dashboard in Grafana.", 
            "title": "Direct Queries"
        }, 
        {
            "location": "/user/direct/#direct-access", 
            "text": "GRACC runs on  Elasticsearch ,\nwhich can be accessed via many  client libraries  and tools.   A read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q.", 
            "title": "Direct Access"
        }, 
        {
            "location": "/user/direct/#curl", 
            "text": "The Elasticsearch  query DSL  is quite complex, but is also very powerful.\nBelow is a relatively simple but non-trivial example of directly querying GRACC from the \ncommand-line via cURL. This query will calculate the number of payload jobs run by \nLIGO in January 2017, and the total wall time they used.   curl 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary '\n{\n     query : {\n         query_string : {\n             query :  VOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01] \n        }\n    },\n     aggs : {\n         walltime : {\n             sum : {\n                 field :  CoreHours \n            }\n        },\n         jobs : {\n             sum : {\n                 field :  Njobs \n            }\n        }\n    },\n     size : 0\n}'  Which might return:  {\n   took  : 2,\n   timed_out  : false,\n   _shards  : {\n     total  : 6,\n     successful  : 6,\n     failed  : 0\n  },\n   hits  : {\n     total  : 68,\n     max_score  : 0.0,\n     hits  : [ ]\n  },\n   aggregations  : {\n     jobs  : {\n       value  : 20626.0\n    },\n     walltime  : {\n       value  : 41323.43916666666\n    }\n  }\n}  Compare to the  VO Summary  dashboard in Grafana.", 
            "title": "cURL"
        }, 
        {
            "location": "/dev-docs/agent-arch/", 
            "text": "Agent Architecture\n\n\nThe agents that coordinate for GRACC\n\n\n\n\nUnlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.\n\n\nComponents\n\n\nThe three major centralized components of GRACC include:\n\n\n\n\nMessage queue: A \nRabbitMQ\n service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.\n\n\nGRACC\n - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.\n\n\nGRACE\n - an \nElasticSearch\n-based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.\n\n\n\n\nOther pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).\n\n\nWe also plan on developing \ngracc-replay\n, a command-line tool for initializing replay of data in the system.  This is meant to:\n\n\n\n\nUpload Gratia raw record tarballs from disk to the message queue.\n\n\nRequest raw data to be resent from a given \nGRACE\n instance to a message queue destination (likely a second \nGRACE\n instance).\n\n\nRequest summary data to be recalculated from a given \nGRACE\n instance to a message queue destination.\n\n\n\n\nAgents\n\n\n\n\nRaw Agent\n\n\nAn agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.\n\n\nSummary Agent\n\n\nThis agent has two responsibilities:\n\n\n\n\nListening to a message queue (\n/grace.\ndb\n.summary\n) for summary records.  It fetches the records from the queue and uploads them into ElasticSearch.  This is implemented in Logstash.\n\n\n\n\nPeriodically request new summaries be made by the Listener agent.  The current behavior of the summarizer is\n\n\n\n\nEvery 15 minutes, we re-summarize the past 7 days of data.\n\n\n\n\n\n\n\n\nThe summary agent also comes with a command line option to re-summarize larger period of times.  The command line is \ngraccsummarizer\n.  The \ngraccsummarizer\n takes a date range as arguments, further help with the command line can be found with the help option.\n\n\nListener Agent\n\n\nA agent running on \nGRACE\n.  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.\n\n\nIt listens on the known queue \n/gracc.\ndb\n.requests\n (as defined on \nMessage Queues\n).  \n\n\nIntegrate OIM Information\n\n\nThe listener agent integrates OIM information for ProjectNames.  For each summary data request it performs these operations:\n\n\n\n\nDownload the Project information in XML from a OIM URL.\n\n\nParse the XML Project information into a hash keyed by the ProjectName.\n\n\nFor each summary record, search for the project information in the OIM hashed data structure, and append the information to the record.\n\n\n\n\nThe attributes copied to the record are:\n\n\n\n\nPI Name\n - The name of the principle investigator for the project.\n\n\nOrganization\n - Institution or organization that the project belogs.\n\n\nDepartment\n - Department inside the institution.\n\n\nField Of Science\n - Field of science inside the Organization.\n\n\n\n\nWe currently do not support the addition of \"Sponsor Campus Grids\".\n\n\nFuture components\n\n\nComponents that will likely be needed in the future include:\n\n\n\n\nGRACE-B\n: Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.\n\n\nGRACE-D\n: A \ndead letter queue\n: a destination for any unparseable or otherwise-rejected records.\n\n\nSome destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a \ncondor_collector\n) and serialize it to a database.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agent-architecture", 
            "text": "The agents that coordinate for GRACC   Unlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#components", 
            "text": "The three major centralized components of GRACC include:   Message queue: A  RabbitMQ  service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.  GRACC  - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.  GRACE  - an  ElasticSearch -based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.   Other pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).  We also plan on developing  gracc-replay , a command-line tool for initializing replay of data in the system.  This is meant to:   Upload Gratia raw record tarballs from disk to the message queue.  Request raw data to be resent from a given  GRACE  instance to a message queue destination (likely a second  GRACE  instance).  Request summary data to be recalculated from a given  GRACE  instance to a message queue destination.", 
            "title": "Components"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agents", 
            "text": "", 
            "title": "Agents"
        }, 
        {
            "location": "/dev-docs/agent-arch/#raw-agent", 
            "text": "An agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.", 
            "title": "Raw Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#summary-agent", 
            "text": "This agent has two responsibilities:   Listening to a message queue ( /grace. db .summary ) for summary records.  It fetches the records from the queue and uploads them into ElasticSearch.  This is implemented in Logstash.   Periodically request new summaries be made by the Listener agent.  The current behavior of the summarizer is   Every 15 minutes, we re-summarize the past 7 days of data.     The summary agent also comes with a command line option to re-summarize larger period of times.  The command line is  graccsummarizer .  The  graccsummarizer  takes a date range as arguments, further help with the command line can be found with the help option.", 
            "title": "Summary Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#listener-agent", 
            "text": "A agent running on  GRACE .  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.  It listens on the known queue  /gracc. db .requests  (as defined on  Message Queues ).", 
            "title": "Listener Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#integrate-oim-information", 
            "text": "The listener agent integrates OIM information for ProjectNames.  For each summary data request it performs these operations:   Download the Project information in XML from a OIM URL.  Parse the XML Project information into a hash keyed by the ProjectName.  For each summary record, search for the project information in the OIM hashed data structure, and append the information to the record.   The attributes copied to the record are:   PI Name  - The name of the principle investigator for the project.  Organization  - Institution or organization that the project belogs.  Department  - Department inside the institution.  Field Of Science  - Field of science inside the Organization.   We currently do not support the addition of \"Sponsor Campus Grids\".", 
            "title": "Integrate OIM Information"
        }, 
        {
            "location": "/dev-docs/agent-arch/#future-components", 
            "text": "Components that will likely be needed in the future include:   GRACE-B : Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.  GRACE-D : A  dead letter queue : a destination for any unparseable or otherwise-rejected records.  Some destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a  condor_collector ) and serialize it to a database.", 
            "title": "Future components"
        }, 
        {
            "location": "/dev-docs/message-queues/", 
            "text": "Message Queues\n\n\nMessage queues used in GRACC\n\n\n\n\nIn AMQP, there is a difference between a \nqueue\n and an \nexchange\n.  Messages delivered on a \nqueue\n are read by a single subscriber; messages delivered on an \nexchange\n will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).\n\n\nWe would like collectors to serve multiple databases (hence the use of an \nexchange\n) while queues are used for messages sent to a database agent.\n\n\nWell known message queues and exchanges used:\n\n\n\n\n/gracc.\ncollector\n.raw\n - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.\n\n\n/grace.\ndb\n.summary\n - A queue that listens for summary records to insert into a specific \ndb\n.  This is used to replicate summary records from other collectors or db's.\n\n\n/grace.\ndb\n.raw\n - Raw record queue for a database instance.\n\n\n/grace.\ndb\n.requests\n - The \nAd Agent\n listens to this queue for requests for raw and summary replications.\n\n\n\n\nHere, \ndb\n is the instance name of a given database install while \ncollector\n is the instance name of an existing Gratia collector.\n\n\nThere are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:\n\n\nRaw Records\n\n\nThese are JSON-formatted documents; the key-value pairs are derived from the OGF \nUsageRecord\n format.  For ease of compatibility with the prior Gratia system, we include an \nnjobs\n attribute if a given record represents more than one job.\n\n\nThe \nRaw Records\n page has more details and the mapping from XML UsageRecord.\n\n\n{\n    \nRecordId\n: \nosg-gw-7.t2.ucsd.edu:35741.2\n,\n    \nCreateTime\n: \n2016-05-27T22:46:46Z\n,\n    \nGlobalJobId\n: \ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nLocalJobId\n: \n185777\n,\n    \nLocalUserId\n: \ncmsuser\n,\n    \nGlobalUsername\n: \ncmsuser@t2.ucsd.edu\n,\n    \nDN\n: \n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User\n,\n    \nVOName\n: \n/cms/Role=production/Capability=NULL\n,\n    \nReportableVOName\n: \ncms\n,\n    \nJobName\n: \nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nMachineName\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nSubmitHost\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nStatus\n: \n0\n,\n    \nStatus_description\n: \nCondor Exit Status\n,\n    \nWallDuration\n: 617,\n    \nCpuDuration\n: 18,\n    \nCpuDuration_system\n: 18,\n    \nCpuDuration_user\n: 0,\n    \nEndTime\n: \n2016-05-27T22:44:08Z\n,\n    \nStartTime\n: \n2016-05-27T22:33:51Z\n,\n    \nHost\n: \ncabinet-1-1-1.t2.ucsd.edu\n,\n    \nNodeCount\n: \n1\n,\n    \nNodeCount_metric\n: \nmax\n,\n    \nProcessors\n: \n1\n,\n    \nProcessors_metric\n: \nmax\n,\n    \nResourceType\n: \nBatch\n,\n    \nProbeName\n: \ncondor:osg-gw-7.t2.ucsd.edu\n,\n    \nSiteName\n: \nUCSDT2-D\n,\n    \nGrid\n: \nOSG\n,\n    \nNjobs\n: \n1\n,\n}\n\n\n\n\n\n\nNote\n\n\nWe consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).\n\n\n\n\nSummary Records\n\n\nThe summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.\n\n\nTODO: copy JSON document here\n\n\n\n\nReplay Requests\n\n\nThe replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.\n\n\nKeys\n:\n\n\n\n\nfrom\n and \nto\n: An ISO 8601 formatted date \n time string that determines the time range beginning and ending, respectively, of the data to be sent.\n\n\nkind\n: What type of records should be resent (valid values are curently \nraw\n or \nsummary\n).\n\n\ndestination\n: An exchange on the same broker where records should be sent.  Should be a string value.\n\n\nrouting_key\n: A routing key to be used when sending the data\n\n\ncontrol\n and \ncontrol_key\n: (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.\n\n\nfilter\n: (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.\n\n\n\n\nExample\n\n\n{\n  \nfrom\n: \n2016-05-10T00:00:00\n,\n  \nto\n: \n2016-05-11T00:00:00\n,\n  \nkind\n: \nraw\n,\n  \ndestination\n: \ngrace.osg.raw\n,\n  \nrouting_key\n: \ngrace.osg.raw\n,\n  \ncontrol\n: \ncontrol-exchange\n,\n  \ncontrol_key\n: \ncontrol_routing_key\n,\n  \nfilter\n: {\n    \nquery\n: {\n      \nquery_string\n: {\n        \nquery\n: \nvo=cms\n\n      }\n    }\n  }\n}", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#message-queues", 
            "text": "Message queues used in GRACC   In AMQP, there is a difference between a  queue  and an  exchange .  Messages delivered on a  queue  are read by a single subscriber; messages delivered on an  exchange  will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).  We would like collectors to serve multiple databases (hence the use of an  exchange ) while queues are used for messages sent to a database agent.  Well known message queues and exchanges used:   /gracc. collector .raw  - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.  /grace. db .summary  - A queue that listens for summary records to insert into a specific  db .  This is used to replicate summary records from other collectors or db's.  /grace. db .raw  - Raw record queue for a database instance.  /grace. db .requests  - The  Ad Agent  listens to this queue for requests for raw and summary replications.   Here,  db  is the instance name of a given database install while  collector  is the instance name of an existing Gratia collector.  There are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#raw-records", 
            "text": "These are JSON-formatted documents; the key-value pairs are derived from the OGF  UsageRecord  format.  For ease of compatibility with the prior Gratia system, we include an  njobs  attribute if a given record represents more than one job.  The  Raw Records  page has more details and the mapping from XML UsageRecord.  {\n     RecordId :  osg-gw-7.t2.ucsd.edu:35741.2 ,\n     CreateTime :  2016-05-27T22:46:46Z ,\n     GlobalJobId :  condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     LocalJobId :  185777 ,\n     LocalUserId :  cmsuser ,\n     GlobalUsername :  cmsuser@t2.ucsd.edu ,\n     DN :  /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User ,\n     VOName :  /cms/Role=production/Capability=NULL ,\n     ReportableVOName :  cms ,\n     JobName :  osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     MachineName :  osg-gw-7.t2.ucsd.edu ,\n     SubmitHost :  osg-gw-7.t2.ucsd.edu ,\n     Status :  0 ,\n     Status_description :  Condor Exit Status ,\n     WallDuration : 617,\n     CpuDuration : 18,\n     CpuDuration_system : 18,\n     CpuDuration_user : 0,\n     EndTime :  2016-05-27T22:44:08Z ,\n     StartTime :  2016-05-27T22:33:51Z ,\n     Host :  cabinet-1-1-1.t2.ucsd.edu ,\n     NodeCount :  1 ,\n     NodeCount_metric :  max ,\n     Processors :  1 ,\n     Processors_metric :  max ,\n     ResourceType :  Batch ,\n     ProbeName :  condor:osg-gw-7.t2.ucsd.edu ,\n     SiteName :  UCSDT2-D ,\n     Grid :  OSG ,\n     Njobs :  1 ,\n}   Note  We consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#summary-records", 
            "text": "The summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.  TODO: copy JSON document here", 
            "title": "Summary Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#replay-requests", 
            "text": "The replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.  Keys :   from  and  to : An ISO 8601 formatted date   time string that determines the time range beginning and ending, respectively, of the data to be sent.  kind : What type of records should be resent (valid values are curently  raw  or  summary ).  destination : An exchange on the same broker where records should be sent.  Should be a string value.  routing_key : A routing key to be used when sending the data  control  and  control_key : (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.  filter : (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.   Example  {\n   from :  2016-05-10T00:00:00 ,\n   to :  2016-05-11T00:00:00 ,\n   kind :  raw ,\n   destination :  grace.osg.raw ,\n   routing_key :  grace.osg.raw ,\n   control :  control-exchange ,\n   control_key :  control_routing_key ,\n   filter : {\n     query : {\n       query_string : {\n         query :  vo=cms \n      }\n    }\n  }\n}", 
            "title": "Replay Requests"
        }, 
        {
            "location": "/dev-docs/raw-records/", 
            "text": "Raw Records\n\n\nRaw records are in JSON format with a schema derived from the \n\nOGF UsageRecord specification\n \nused by GRACC's predecessor Gratia.\n\n\nRequirements\n\n\nTo maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:\n\n\n\n\nResourceType\n\n\nCommonName\n\n\nVOName\n\n\nReportableVOName\n\n\nProjectName\n\n\nEndTime\n\n\nCpuDuration\n\n\nWallDuration\n\n\nProcessors\n\n\n...?\n\n\n\n\nDates and Times\n\n\nAll times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.\n\n\nConverting XML JobUsageRecord\n\n\nThe mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.\n\n\nThe raw XML record is stored in the \nRawXML\n field, to allow for later reference \nand remapping.\n\n\nIdentity Groups\n\n\nIdentity groups are flattened by moving their sub-elements to the top level:\n\n\n\n\nRecordIdentity\n\n\nRecordId\n\n\nCreateTime\n\n\nJobIdentity\n\n\nGlobalJobId\n\n\nLocalJobId\n\n\nProcessId (array)\n\n\nUserIdentity\n\n\nLocalUserId\n\n\nGlobalUsername\n\n\nCommonName\n\n\nDN\n\n\nVOName\n\n\nReportableVOName\n\n\n\n\nDurations\n\n\nDuration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:\n\n\n\n\nCpuDuration (combined)\n\n\nCpuDuration_user\n\n\nCpuDuration_system\n\n\nWallDuration\n\n\n\n\nResource\n\n\nResources are transformed into a \ndescription\n:\nvalue\n map.\nThe description is transformed to make it a valid field name \n(spaces and dots are converted to dashes) prefixed with \nResource_\n. \nOther properties are flattened as \nResource_\ndescription\n_\nproperty_name\n:\nproperty_value\n.\n\n\nThe special Resource \nResourceType\n is moved directly to the top level. ResourceType \n\nBatchPilot\n is renamed \nPayload\n due to the former being misleading.\n\n\nTimeDuration and TimeInstant elements are likewise put in \ntype\n:\nvalue\n maps \nwith their respective prefixes. Durations are converted to seconds, discrete times \nare ISO8601 strings.\n\n\nOther\n\n\nAny other elements are directly included in the top level. Properties of those elements are moved to fields named as \nelement\n_\nproperty\n, e.g. \nJobName_description\n.\n\n\nExample XML Record\n\n\nJobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\"\n\n    \nRecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/\n\n    \nJobIdentity\n\n        \nGlobalJobId\ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/GlobalJobId\n\n        \nLocalJobId\n185777\n/LocalJobId\n\n    \n/JobIdentity\n\n    \nUserIdentity\n\n        \nLocalUserId\ncmsuser\n/LocalUserId\n\n        \nGlobalUsername\ncmsuser@t2.ucsd.edu\n/GlobalUsername\n\n        \nDN\n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\n/DN\n\n        \nVOName\n/cms/Role=production/Capability=NULL\n/VOName\n\n        \nReportableVOName\ncms\n/ReportableVOName\n\n    \n/UserIdentity\n\n    \nJobName\nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/JobName\n\n    \nMachineName\nosg-gw-7.t2.ucsd.edu\n/MachineName\n\n    \nSubmitHost\nosg-gw-7.t2.ucsd.edu\n/SubmitHost\n\n    \nStatus urwg:description=\"Condor Exit Status\"\n0\n/Status\n\n    \nWallDuration urwg:description=\"Was entered in seconds\"\nPT10M17.0S\n/WallDuration\n\n    \nTimeDuration urwg:type=\"RemoteUserCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeD\n    \nTimeDuration urwg:type=\"RemoteSysCpu\"\nPT18.0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"LocalSysCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CumulativeSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedTime\"\nPT10M17.0S\n/TimeDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\"\nPT18.0S\n/CpuDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\"\nPT0S\n/CpuDuration\n\n    \nEndTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:44:08Z\n/EndTime\n\n    \nStartTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:33:51Z\n/StartTime\n\n    \nHost primary=\"true\"\ncabinet-1-1-1.t2.ucsd.edu\n/Host\n\n    \nQueue urwg:description=\"Condor's JobUniverse field\"\n5\n/Queue\n\n    \nNodeCount urwg:metric=\"max\"\n1\n/NodeCount\n\n    \nProcessors urwg:metric=\"max\"\n1\n/Processors\n\n    \nResource urwg:description=\"CondorMyType\"\nJob\n/Resource\n\n    \nResource urwg:description=\"AccountingGroup\"\ngroup_cmsprod.cmsuser\n/Resource\n\n    \nResource urwg:description=\"ExitBySignal\"\nfalse\n/Resource\n\n    \nResource urwg:description=\"ExitCode\"\n0\n/Resource\n\n    \nResource urwg:description=\"condor.JobStatus\"\n4\n/Resource\n\n    \nNetwork urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\"\n0\n/Network\n\n    \nProbeName\ncondor:osg-gw-7.t2.ucsd.edu\n/ProbeName\n\n    \nSiteName\nUCSDT2-D\n/SiteName\n\n    \nGrid\nOSG\n/Grid\n\n    \nNjobs\n1\n/Njobs\n\n    \nResource urwg:description=\"ResourceType\"\nBatch\n/Resource\n\n\n/JobUsageRecord\n\n\n\n\nExample Corresponding JSON\n\n\n{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration_CommittedSuspensionTime\": 0,\n    \"TimeDuration_CommittedTime\": 617,\n    \"TimeDuration_CumulativeSuspensionTime\": 0,\n    \"TimeDuration_LocalSysCpu\": 0,\n    \"TimeDuration_LocalUserCpu\": 0,\n    \"TimeDuration_RemoteSysCpu\": 18,\n    \"TimeDuration_RemoteUserCpu\": 0,\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\",\n    \"Resource_CondorMyType\": \"Job\",\n    \"Resource_ExitBySignal\": \"false\",\n    \"Resource_ExitCode\": \"0\",\n    \"Resource_condor-JobStatus\": \"4\",\n    \"ResourceType\": \"Batch\",\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#raw-records", 
            "text": "Raw records are in JSON format with a schema derived from the  OGF UsageRecord specification  \nused by GRACC's predecessor Gratia.", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#requirements", 
            "text": "To maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:   ResourceType  CommonName  VOName  ReportableVOName  ProjectName  EndTime  CpuDuration  WallDuration  Processors  ...?", 
            "title": "Requirements"
        }, 
        {
            "location": "/dev-docs/raw-records/#dates-and-times", 
            "text": "All times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.", 
            "title": "Dates and Times"
        }, 
        {
            "location": "/dev-docs/raw-records/#converting-xml-jobusagerecord", 
            "text": "The mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.  The raw XML record is stored in the  RawXML  field, to allow for later reference \nand remapping.", 
            "title": "Converting XML JobUsageRecord"
        }, 
        {
            "location": "/dev-docs/raw-records/#identity-groups", 
            "text": "Identity groups are flattened by moving their sub-elements to the top level:   RecordIdentity  RecordId  CreateTime  JobIdentity  GlobalJobId  LocalJobId  ProcessId (array)  UserIdentity  LocalUserId  GlobalUsername  CommonName  DN  VOName  ReportableVOName", 
            "title": "Identity Groups"
        }, 
        {
            "location": "/dev-docs/raw-records/#durations", 
            "text": "Duration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:   CpuDuration (combined)  CpuDuration_user  CpuDuration_system  WallDuration", 
            "title": "Durations"
        }, 
        {
            "location": "/dev-docs/raw-records/#resource", 
            "text": "Resources are transformed into a  description : value  map.\nThe description is transformed to make it a valid field name \n(spaces and dots are converted to dashes) prefixed with  Resource_ . \nOther properties are flattened as  Resource_ description _ property_name : property_value .  The special Resource  ResourceType  is moved directly to the top level. ResourceType  BatchPilot  is renamed  Payload  due to the former being misleading.  TimeDuration and TimeInstant elements are likewise put in  type : value  maps \nwith their respective prefixes. Durations are converted to seconds, discrete times \nare ISO8601 strings.", 
            "title": "Resource"
        }, 
        {
            "location": "/dev-docs/raw-records/#other", 
            "text": "Any other elements are directly included in the top level. Properties of those elements are moved to fields named as  element _ property , e.g.  JobName_description .", 
            "title": "Other"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-xml-record", 
            "text": "JobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\" \n     RecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/ \n     JobIdentity \n         GlobalJobId condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /GlobalJobId \n         LocalJobId 185777 /LocalJobId \n     /JobIdentity \n     UserIdentity \n         LocalUserId cmsuser /LocalUserId \n         GlobalUsername cmsuser@t2.ucsd.edu /GlobalUsername \n         DN /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba /DN \n         VOName /cms/Role=production/Capability=NULL /VOName \n         ReportableVOName cms /ReportableVOName \n     /UserIdentity \n     JobName osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /JobName \n     MachineName osg-gw-7.t2.ucsd.edu /MachineName \n     SubmitHost osg-gw-7.t2.ucsd.edu /SubmitHost \n     Status urwg:description=\"Condor Exit Status\" 0 /Status \n     WallDuration urwg:description=\"Was entered in seconds\" PT10M17.0S /WallDuration \n     TimeDuration urwg:type=\"RemoteUserCpu\" PT0S /TimeDuration \n     TimeD\n     TimeDuration urwg:type=\"RemoteSysCpu\" PT18.0S /TimeDuration \n     TimeDuration urwg:type=\"LocalSysCpu\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CumulativeSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedTime\" PT10M17.0S /TimeDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\" PT18.0S /CpuDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\" PT0S /CpuDuration \n     EndTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:44:08Z /EndTime \n     StartTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:33:51Z /StartTime \n     Host primary=\"true\" cabinet-1-1-1.t2.ucsd.edu /Host \n     Queue urwg:description=\"Condor's JobUniverse field\" 5 /Queue \n     NodeCount urwg:metric=\"max\" 1 /NodeCount \n     Processors urwg:metric=\"max\" 1 /Processors \n     Resource urwg:description=\"CondorMyType\" Job /Resource \n     Resource urwg:description=\"AccountingGroup\" group_cmsprod.cmsuser /Resource \n     Resource urwg:description=\"ExitBySignal\" false /Resource \n     Resource urwg:description=\"ExitCode\" 0 /Resource \n     Resource urwg:description=\"condor.JobStatus\" 4 /Resource \n     Network urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\" 0 /Network \n     ProbeName condor:osg-gw-7.t2.ucsd.edu /ProbeName \n     SiteName UCSDT2-D /SiteName \n     Grid OSG /Grid \n     Njobs 1 /Njobs \n     Resource urwg:description=\"ResourceType\" Batch /Resource  /JobUsageRecord", 
            "title": "Example XML Record"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-corresponding-json", 
            "text": "{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration_CommittedSuspensionTime\": 0,\n    \"TimeDuration_CommittedTime\": 617,\n    \"TimeDuration_CumulativeSuspensionTime\": 0,\n    \"TimeDuration_LocalSysCpu\": 0,\n    \"TimeDuration_LocalUserCpu\": 0,\n    \"TimeDuration_RemoteSysCpu\": 18,\n    \"TimeDuration_RemoteUserCpu\": 0,\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\",\n    \"Resource_CondorMyType\": \"Job\",\n    \"Resource_ExitBySignal\": \"false\",\n    \"Resource_ExitCode\": \"0\",\n    \"Resource_condor-JobStatus\": \"4\",\n    \"ResourceType\": \"Batch\",\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Example Corresponding JSON"
        }, 
        {
            "location": "/dev-docs/install-grace-db/", 
            "text": "Installing GRACE\n\n\nThe GRACE database service consists of:\n\n\n\n\nElasticSearch as a datastore.\n\n\ngrace-raw-listener\n: Listens to the raw records from the collector\n\n\ngrace-summary-listener\n: Listens for and requests summary records.\n\n\ngrace-request-listener\n: Listens for replay and summarization requests.\n\n\n\n\nSee the \nagent architecture docs\n for more information.\n\n\nAdditionally, for monitoring and visualization, one commonly installs\nthe following external components.\n\n\n\n\nInfluxDB (primarily for monitoring ES performance)\n\n\nGrafana (visualization)\n\n\nKibana (visualization)\n\n\n\n\nDependencies\n\n\nThis document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.\n\n\nWe also assume that the OSG repos and \nyum\n priorities have been \nappropriately configured\n.\n\n\nInstallation\n\n\nThe relevant components can be pulled in via a meta-RPM:\n\n\nyum install --enablerepo=osg-development osg-grace\n\n\n\n\nConfiguration\n\n\nConfiguration files are kept in \n/etc/gracc/config.d\n and \n/usr/share/gracc/config.d\n; files in this directory are read in lexigraphical order.  The file format is \nTOML\n.\n\n\nYou will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in \n/usr/share/gracc/config.d\n: these will be overwritten on upgrade.  Instead, start by editing the samples in \n/etc\n.\n\n\nMost strings will expand the text %I to the \"instance name\".  So, for the \nosg\n instance, the following,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n\n\n\nis equivalent to\n\n\n[AMQP]\nexchange = \ngracc.osg.raw\n\nqueue = \ngrace.osg.raw\n\n\n\n\n\nFurther, we can have specific instance overrides.  Hence,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n[AMQP.osg]\nexchange = \ngracc.osg-test.raw\n\n\n\n\n\nis equivalent to:\n\n\n[AMQP]\nexchange = \ngracc.osg-test.raw\n\nqueue = \ngracc.osg.raw\n\n\n\n\n\nRunning services\n\n\nTo configure GRACC to start at boot, you will need to do the following for the \nosg\n instance:\n\n\nln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload\n\n\n\n\nMultiple instance can be run by editing the instance name above.\n\n\nFinally, these services can be started via the typical system management commands:\n\n\nsystemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg\n\n\n\n\nLog files\n\n\nBy default, log files go into \n/var/log/gracc\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installing-grace", 
            "text": "The GRACE database service consists of:   ElasticSearch as a datastore.  grace-raw-listener : Listens to the raw records from the collector  grace-summary-listener : Listens for and requests summary records.  grace-request-listener : Listens for replay and summarization requests.   See the  agent architecture docs  for more information.  Additionally, for monitoring and visualization, one commonly installs\nthe following external components.   InfluxDB (primarily for monitoring ES performance)  Grafana (visualization)  Kibana (visualization)", 
            "title": "Installing GRACE"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#dependencies", 
            "text": "This document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.  We also assume that the OSG repos and  yum  priorities have been  appropriately configured .", 
            "title": "Dependencies"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installation", 
            "text": "The relevant components can be pulled in via a meta-RPM:  yum install --enablerepo=osg-development osg-grace", 
            "title": "Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#configuration", 
            "text": "Configuration files are kept in  /etc/gracc/config.d  and  /usr/share/gracc/config.d ; files in this directory are read in lexigraphical order.  The file format is  TOML .  You will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in  /usr/share/gracc/config.d : these will be overwritten on upgrade.  Instead, start by editing the samples in  /etc .  Most strings will expand the text %I to the \"instance name\".  So, for the  osg  instance, the following,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw   is equivalent to  [AMQP]\nexchange =  gracc.osg.raw \nqueue =  grace.osg.raw   Further, we can have specific instance overrides.  Hence,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw \n\n[AMQP.osg]\nexchange =  gracc.osg-test.raw   is equivalent to:  [AMQP]\nexchange =  gracc.osg-test.raw \nqueue =  gracc.osg.raw", 
            "title": "Configuration"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#running-services", 
            "text": "To configure GRACC to start at boot, you will need to do the following for the  osg  instance:  ln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload  Multiple instance can be run by editing the instance name above.  Finally, these services can be started via the typical system management commands:  systemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg", 
            "title": "Running services"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#log-files", 
            "text": "By default, log files go into  /var/log/gracc .", 
            "title": "Log files"
        }, 
        {
            "location": "/dev-docs/backups/", 
            "text": "Backup Configuration\n\n\nBackup Sources\n\n\nGRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system.  The \nGRACC Archiver\n listens to the raw RabbitMQ exchanges.  It listens to both the \ngracc.osg.raw\n and the \ngracc.osg-transfer.raw\n exchanges.\n\n\nThe archive agent stores the records into a tar.gz file located in \n/var/lib/graccarchive/sandbox\n.  On new days (or agent crashes) the tar.gz files are atomically copied to \n/var/lib/graccarchive/output\n.  The transfer archiver similarily stores files in \n/var/lib/graccarchive/sandbox-transfer\n and \n/var/lib/graccarchive/output-transfer\n.\n\n\nThe archive agent is configured in \n/etc/graccarchive/conf\n.  It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time.\n\n\nBackup Location\n\n\nThe backups are copied to FNAL by the \ngracc-backup\n tool.  This uses SystemD timer and service files to periodically copy the output \n.tar.gz\n files to FNAL.  The final destination (gsiftp) of the files is configured in the *.service files with the tool\n\n\nRestore Operation\n\n\nThe restore operation uses the \ngraccunarchiver\n tool distributed with the GRACC Archiver agent.  The workflow of a restore is:\n\n\n\n\nCopy the backup file from the backup location.  You will likely need to use \nglobus-url-copy\n in order to copy the files back from the backup location.\n\n\n\n\nRun the \ngraccunarchiver\n tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters.\n\n\ngraccunarchiver \nrabbitmq_url\n gracc.osg.raw gracc-2017-04-04.tar.gz\n\n\n\n\n\n\n\nAfter restoring the raw jobs and transfers, it may be necessary to re-summarize the restored time-period with the \ngraccsummarizer\n tool.\n\n\n\n\n\n\n\n\nNote\n\n\nIf the backup tar.gz file was created during a crash of the agent or system, it's possible that the tar.gz end may be corrupted and you may see CRC or other errors.  The vast majority of the records are fine, but the last few may be corrupted and un-retrievable.", 
            "title": "Backup Documentation"
        }, 
        {
            "location": "/dev-docs/backups/#backup-configuration", 
            "text": "", 
            "title": "Backup Configuration"
        }, 
        {
            "location": "/dev-docs/backups/#backup-sources", 
            "text": "GRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system.  The  GRACC Archiver  listens to the raw RabbitMQ exchanges.  It listens to both the  gracc.osg.raw  and the  gracc.osg-transfer.raw  exchanges.  The archive agent stores the records into a tar.gz file located in  /var/lib/graccarchive/sandbox .  On new days (or agent crashes) the tar.gz files are atomically copied to  /var/lib/graccarchive/output .  The transfer archiver similarily stores files in  /var/lib/graccarchive/sandbox-transfer  and  /var/lib/graccarchive/output-transfer .  The archive agent is configured in  /etc/graccarchive/conf .  It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time.", 
            "title": "Backup Sources"
        }, 
        {
            "location": "/dev-docs/backups/#backup-location", 
            "text": "The backups are copied to FNAL by the  gracc-backup  tool.  This uses SystemD timer and service files to periodically copy the output  .tar.gz  files to FNAL.  The final destination (gsiftp) of the files is configured in the *.service files with the tool", 
            "title": "Backup Location"
        }, 
        {
            "location": "/dev-docs/backups/#restore-operation", 
            "text": "The restore operation uses the  graccunarchiver  tool distributed with the GRACC Archiver agent.  The workflow of a restore is:   Copy the backup file from the backup location.  You will likely need to use  globus-url-copy  in order to copy the files back from the backup location.   Run the  graccunarchiver  tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters.  graccunarchiver  rabbitmq_url  gracc.osg.raw gracc-2017-04-04.tar.gz    After restoring the raw jobs and transfers, it may be necessary to re-summarize the restored time-period with the  graccsummarizer  tool.     Note  If the backup tar.gz file was created during a crash of the agent or system, it's possible that the tar.gz end may be corrupted and you may see CRC or other errors.  The vast majority of the records are fine, but the last few may be corrupted and un-retrievable.", 
            "title": "Restore Operation"
        }, 
        {
            "location": "/ops/services/", 
            "text": "GRACC Services\n\n\nThese are the services the comprise the GRACC system, and details on\nwhere and how they are currently deployed on GRACE. \n\n\n\n\nItems marked with !! should be redesigned for production.\n\n\nItems marked with ?? should be evaluated for removal.\n  (note: FIFE services will be moved to another Elasticsearch cluster \"soon\")\n\n\n\n\nExternal\n\n\nRabbitMQ\n\n\n\n\nRunning at the GOC\n\n\nITB: event-itb.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin)\n\n\nProduction: event.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin)\n\n\n\n\nHead Node (\ngratiav2-1\n)\n\n\nOpenstack limits external access to ssh (:22) and http (:80 and :443).\n\n\nElasticsearch (ingest node)\n\n\n\n\nInstalled from official elastic repository\n\n\nConfig in /etc/elasticsearch\n\n\nRunning under systemd: \nelasticsearch.service\n\n\nListening at:\n\n\nhttp://0.0.0.0:9200 and :9300\n\n\nhttps://gracc.opensciencegrid.org/e/\n\n\n?? https://fifemon-es.fnal.gov\n\n\n\n\n\n\nExternal access requires certificate from GRACC private CA\n\n\nCerts and keys in \n/etc/nginx/certs\n\n\nRun \nmake_cert COMMON_NAME\n to generate and sign a new certificate\n\n\n\n\n\n\nnginx allows very limited read-only access through public endpoints\n\n\n\n\nElasticsearch (query node)\n\n\n\n\n!! Manual copy of ingest node (\ns/elasticsearch/elasticsearch-ro/\n)\n\n\nConfig in \n/etc/elasticsearch-ro\n\n\nRunning under systemd: \nelasticsearch-ro.service\n\n\nListening at:\n\n\nhttp://localhost:9201\n\n\nhttps://gracc.opensciencegrid.org/q/\n\n\nIncludes \nreadonlyrest\n plugin to allow limited unauthenticated read-only access \n  from Kibana, Grafana, and to public queries. Configured in \nelasticsearch.yml\n\n  https://readonlyrest.com\n\n\n\n\nGRACC Collectors and Stash agents\n\n\nGRACC Collectors (\ngracc-collector\n) accept records in formats provided by Gratia \nprobes and collectors, transform them into GRACC JSON documents, and send them to \nthe appropriate RabbitMQ exchange, which by convention is \ngracc.$STREAM.raw\n.\n\n\nRaw Stash agents (\ngracc-stash-raw\n) create a queue in RabbitMQ that is bound the the appropriate\nraw exchange, do minor manipulations to the documents (add checksum, calculate derived fields),\nand index the documents into elasticsearch (\ngracc.$STREAM.raw$N-$YYYY.$MM\n), where \n$N\n is a \nmonotonic schema version number. !! A cron job, currently running under kretzke, generates aliases \nin Elasticsearch each month from \ngracc.$STREAM.raw$N-$YYYY.$MM\n to \ngracc.$STREAM.raw-$YYYY.$MM\n.\n\n\nSummary Stash agents (\ngracc-stash-summary\n) create a queue in RabbitMQ that is bound the the appropriate\nsummary exchange, do minor manipulations to the documents (add checksum, calculate derived fields),\nand index the documents into elasticsearch (\ngracc.$STREAM.summary$N-$YYYY\n), where \n$N\n is a \nmonotonic schema version number. !! Aliases are manually cretated in Elasticsearch from \n\ngracc.$STREAM.summary$N-$YYYY\n to \ngracc.$STREAM.summary\n (note just \none\n alias for all years).\n\n\n\n\nRunning under docker, via docker-compose\n\n\nConfig in \n/etc/gracc/docker/$STREAM\n\n\nWithin each directory is:\n\n\ndocker-compose.yml\n - defines services for the stream\n\n\n.env\n - common environment variables for the stream's services\n\n\n\n\n\n\n\n\nStreams\n\n\n\n\ngracc.osg\n - \"main\" job stream\n\n\nCollector listening at:\n\n\nhttp://localhost:8180\n\n\nhttp://gracc.opensciencegrid.org/gracc/osg\n\n\n\n\n\n\n\n\n\n\ngracc.osg-itb\n - ITB test stream\n\n\nCollector listening at:\n\n\nhttp://localhost:8181\n\n\nhttp://gracc.opensciencegrid.org/gracc/osg-itb\n\n\n\n\n\n\n\n\n\n\ngracc.osg-transfer\n - transfers stream\n\n\nCollector listening at:\n\n\nhttp://localhost:8182\n\n\nhttp://gracc.opensciencegrid.org/gracc/osg-transfer\n\n\n\n\n\n\n\n\n\n\n\n\nGRACC Request Agent\n\n\nThe Request agent (\ngracc-request\n) listens on a RabbitMQ exchange (\ngracc.$STREAM.requests\n) \nfor requests for \"replay\" or \"summarization\" of raw records for a given time period. The request \nincludes a RabbitMQ exhchange the records should be sent to.\n\n\n\n\n!! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/)\n\n\nRunning under systemd: \ngraccreq.service\n\n\nConfig in \n/etc/graccreq/config.d/gracc-request.toml\n\n\n\n\nGRACC Summary Agent\n\n\nThe Summary agent (\ngracc-summary\n) periodically requests summarizations, and has a CLI to manually\nsummarize periods.\n\n\n\n\n!! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/)\n\n\nRunning under systemd: \n\n\nEvery 15 minutes, resummarize the last 7 days: \ngraccsumperiodic.service\n and \ngraccsumperiodic.timer\n\n\nEvery 24 hours, resummarize the last 365 days: \ngraccsumperiodicyearly.service\n and \ngraccsumperiodicyearly.timer\n\n\nConfig in \n/etc/graccsum/config.d/gracc-summary.toml\n\n\n\n\nAPEL reporting\n\n\n\n\nSource at \nOSG docker\n \n\n\ndocker {pull, run} opensciencegrid/gracc-apel\n\n\n\n\n\n\nRunning under systemd: \ngracc-apel.service\n and \ngracc-apel.timer\n\n\nRegistered service certificate with \nAPEL admins\n needed \n\n\nin docker we use:\n\n$ openssl x509 -in /etc/grid-security/apel/apelcert.pem -noout -subject\n    subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=apel/hcc-grace.unl.edu\n\n\nSELinux prevents inheritance of certificates, so this must be set on docker master node: \nchcon -Rt svirt_sandbox_file_t /etc/grid-security/apel/\n\n\n\n\n\n\n\n\nConfiguring GRACC-APEL in systemd\n\n\n\n\ngracc-apel.service\n configure, enable, start, test\n\n\n\n\n    $ cat /lib/systemd/system/gracc-apel.service \n      [Unit]\n      Description=GRACC APEL reporting Docker container\n\n      [Service]\n      Type=oneshot\n      ExecStart=/bin/docker run -v /etc/grid-security/apel/apelcert.pem:/etc/grid-security/apel/apelcert.pem -v /etc/grid-security/apel/apelkey.pem:/etc/grid-security/apel/apelkey.pem opensciencegrid/gracc-apel\n\n    $ systemctl enabled gracc-apel.service\n    $ systemctl start gracc-apel.service\n    $ systemctl status gracc-apel.service\n\n\n\n\nTo run periodically:\n* \ngracc-apel.timer\n configure, enable, start\n\n\n    $ cat /lib/systemd/system/gracc-apel.timer \n    [Unit]\n    Description=Run GRACC-to-APEL reporting script\n\n    [Timer]\n    # Explicitly declare service that this timer is responsible for\n    Unit=gracc-apel.service\n    # Runs 'gracc-apel' relative to when the *timer-unit* has been activated\n    OnActiveSec=1hour\n    # Runs 'gracc-apel' relative to when *service-unit* was last deactivated\n    OnUnitInactiveSec=1hour\n\n    # Randomize runtime by a small amount each run.\n    RandomizedDelaySec=2min\n\n    [Install]\n    WantedBy=timers.target\n    $ systemctl enabled gracc-apel.timer\n    $ systemctl start gracc-apel.timer\n    $ systemctl status gracc-apel.timer\n\n\n\n\n* To check if `gracc-timer.timer` runs:\n\n\n\n    $ systemctl list-timers *apel*\n    NEXT                         LEFT       LAST                         PASSED    UNIT             ACTIVATES\n    Mon 2017-05-22 17:48:37 CDT  21min left Mon 2017-05-22 16:46:40 CDT  40min ago gracc-apel.timer gracc-apel.service\n\n\n\n\nGrafana\n\n\n\n\nInstalled from official Grafana yum repo\n\n\nRunning under systemd: \ngrafana-server.service\n\n\nConfig in \n/etc/grafana\n\n\nData in \n/var/lib/grafana\n\n\nListening at:\n\n\nhttp://localhost:3000\n\n\nhttps://gracc.opensciencegrid.org/\n\n\n\n\nKibana\n\n\n\n\nInstalled from official Elasticsearch yum repo\n\n\nRunning under systemd: \nkibana.service\n\n\nConfig in /etc/kibana\n\n\nListening at:\n\n\nhttp://localhost:5601\n\n\nhttps://gracc.opensciencegrid.org/kibana/\n\n\nreadonlyrest\n Elasticsearch plugin controls access, write actions require authentication\n  with basic auth, configured in \n/etc/elasticsearch-ro/elasticsearch.yml\n.\n\n\n\n\nPrometheus\n\n\nPrometheus is used for monitoring the nodes and services.\n\n\n\n\n!! Installed in \n/opt/prometheus\n\n\nConfig in \n/etc/prometheus\n\n\nData in \n/data/prometheus\n\n\nRunning under systemd: \nprometheus.service\n\n\nListening at:\n\n\nhttp://localhost:9090\n\n\nhttps://gracc.opensciencegrid.org/prometheus \n\n\n!! external access requires basic auth, \n/etc/nginx/conf.d/kibana.htpasswd\n\n\nlimited unauthenticated access to query endpoint\n\n\n\n\nPrometheus Exporters\n\n\nPrometheus exporters collect data from services and present them for collection by Prometheus.\n\n\nRabbitMQ Exporter\n\n\n\n\n!! manually run under docker (to be moved to docker-compose)\n\n\nconfiguration via env var\n\n\nRABBIT_URL=https://event-itb.grid.iu.edu:15671\n\n\nRABBIT_USER=$USER\n\n\nRABBIT_PASSWORD=$PASSWORD\n\n\n\n\n\n\nListening at http://localhost:9111\n\n\n\n\nNode Exporter\n\n\n\n\nInstalled at \n/opt/prometheus\n\n\nRun with systemd: \nprometheus-node-exporter.service\n\n\nListening at http://0.0.0.0:9100\n\n\n\n\nGraphite Exporter\n\n\nThe Graphite exporter accepts time-series data in graphite format and exposes it for \ncollection by prometheus.\n\n\n\n\nRunning under docker\n\n\nListening at:\n\n\nhttp://0.0.0.0:9100 (publish to prometheus)\n\n\nhttps://0.0.0.0:2003 (collect from graphite)\n\n\n\n\n\n\n\n\nNginx\n\n\nAll public services are proxied through nginx.\n\n\n\n\nInstalled from EPEL\n\n\nPrimary config in \n/etc/nginx/conf.d/default.conf\n\n\nRunning under systemd: \nnginx.service\n\n\nListening at:\n\n\nhttp://0.0.0.0:80\n\n\nhttp://0.0.0.0:443\n\n\nhttp://gracc.opensciencegrid.org\n\n\nhttps://gracc.opensciencegrid.org\n\n\n\n\nDocker\n\n\n\n\nRunning under systemd: \ndocker.service\n\n\nExtra config in \n/etc/systemd/system/docker.service.d\n\n\nContainer logs sent to journald; e.g. \nsudo journalctl CONTAINER_NAME=graccosg_gracc-collector_1\n\n\n\n\nPortainer\n\n\nPortainer is a web-based utility for monitoring and managing containers.\n\n\n\n\nrunning under docker\n\n\nlistening on http://0.0.0.0:9000\n\n\n!! not proxied through nginx, need to tunnel, e.g. \nssh -L 9000:localhost:9000 gracc.opensciencegrid.org\n\n\n\n\nData Nodes (\ngratiav2-2\n though \n-5\n)\n\n\nAnsible is set up on the head node to perform operations across all data \nnodes, e.g. \nsudo ansible elasticsearch -a 'uptime'\n. Configuration in\n\n/etc/ansible\n.\n\n\nElasticsearch\n\n\n\n\nInstalled from official elastic repository\n\n\nConfig in \n/etc/elasticsearch\n\n\nData in  \n/data/elasticsearch\n and \n/data2/elasticsearch\n\n\nListening at http://0.0.0.0:9200 and :9300\n\n\n\n\nPrometheus Exporters\n\n\nNode Exporter\n\n\n\n\nInstalled at \n/opt/prometheus\n\n\nRun with systemd: \nprometheus-elasticsearch-exporter.service\n\n\nListening at http://0.0.0.0:9100\n\n\n\n\nElasticsearch Exporter\n\n\n\n\nInstalled at \n/opt/prometheus\n\n\nRun with systemd: \nprometheus-node-exporter.service\n\n\nListening at http://0.0.0.0:9108", 
            "title": "Operations"
        }, 
        {
            "location": "/ops/services/#gracc-services", 
            "text": "These are the services the comprise the GRACC system, and details on\nwhere and how they are currently deployed on GRACE.    Items marked with !! should be redesigned for production.  Items marked with ?? should be evaluated for removal.\n  (note: FIFE services will be moved to another Elasticsearch cluster \"soon\")", 
            "title": "GRACC Services"
        }, 
        {
            "location": "/ops/services/#external", 
            "text": "", 
            "title": "External"
        }, 
        {
            "location": "/ops/services/#rabbitmq", 
            "text": "Running at the GOC  ITB: event-itb.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin)  Production: event.grid.iu.edu:5672, :5671 (TLS), :15671 (HTTPS admin)", 
            "title": "RabbitMQ"
        }, 
        {
            "location": "/ops/services/#head-node-gratiav2-1", 
            "text": "Openstack limits external access to ssh (:22) and http (:80 and :443).", 
            "title": "Head Node (gratiav2-1)"
        }, 
        {
            "location": "/ops/services/#elasticsearch-ingest-node", 
            "text": "Installed from official elastic repository  Config in /etc/elasticsearch  Running under systemd:  elasticsearch.service  Listening at:  http://0.0.0.0:9200 and :9300  https://gracc.opensciencegrid.org/e/  ?? https://fifemon-es.fnal.gov    External access requires certificate from GRACC private CA  Certs and keys in  /etc/nginx/certs  Run  make_cert COMMON_NAME  to generate and sign a new certificate    nginx allows very limited read-only access through public endpoints", 
            "title": "Elasticsearch (ingest node)"
        }, 
        {
            "location": "/ops/services/#elasticsearch-query-node", 
            "text": "!! Manual copy of ingest node ( s/elasticsearch/elasticsearch-ro/ )  Config in  /etc/elasticsearch-ro  Running under systemd:  elasticsearch-ro.service  Listening at:  http://localhost:9201  https://gracc.opensciencegrid.org/q/  Includes  readonlyrest  plugin to allow limited unauthenticated read-only access \n  from Kibana, Grafana, and to public queries. Configured in  elasticsearch.yml \n  https://readonlyrest.com", 
            "title": "Elasticsearch (query node)"
        }, 
        {
            "location": "/ops/services/#gracc-collectors-and-stash-agents", 
            "text": "GRACC Collectors ( gracc-collector ) accept records in formats provided by Gratia \nprobes and collectors, transform them into GRACC JSON documents, and send them to \nthe appropriate RabbitMQ exchange, which by convention is  gracc.$STREAM.raw .  Raw Stash agents ( gracc-stash-raw ) create a queue in RabbitMQ that is bound the the appropriate\nraw exchange, do minor manipulations to the documents (add checksum, calculate derived fields),\nand index the documents into elasticsearch ( gracc.$STREAM.raw$N-$YYYY.$MM ), where  $N  is a \nmonotonic schema version number. !! A cron job, currently running under kretzke, generates aliases \nin Elasticsearch each month from  gracc.$STREAM.raw$N-$YYYY.$MM  to  gracc.$STREAM.raw-$YYYY.$MM .  Summary Stash agents ( gracc-stash-summary ) create a queue in RabbitMQ that is bound the the appropriate\nsummary exchange, do minor manipulations to the documents (add checksum, calculate derived fields),\nand index the documents into elasticsearch ( gracc.$STREAM.summary$N-$YYYY ), where  $N  is a \nmonotonic schema version number. !! Aliases are manually cretated in Elasticsearch from  gracc.$STREAM.summary$N-$YYYY  to  gracc.$STREAM.summary  (note just  one  alias for all years).   Running under docker, via docker-compose  Config in  /etc/gracc/docker/$STREAM  Within each directory is:  docker-compose.yml  - defines services for the stream  .env  - common environment variables for the stream's services", 
            "title": "GRACC Collectors and Stash agents"
        }, 
        {
            "location": "/ops/services/#streams", 
            "text": "gracc.osg  - \"main\" job stream  Collector listening at:  http://localhost:8180  http://gracc.opensciencegrid.org/gracc/osg      gracc.osg-itb  - ITB test stream  Collector listening at:  http://localhost:8181  http://gracc.opensciencegrid.org/gracc/osg-itb      gracc.osg-transfer  - transfers stream  Collector listening at:  http://localhost:8182  http://gracc.opensciencegrid.org/gracc/osg-transfer", 
            "title": "Streams"
        }, 
        {
            "location": "/ops/services/#gracc-request-agent", 
            "text": "The Request agent ( gracc-request ) listens on a RabbitMQ exchange ( gracc.$STREAM.requests ) \nfor requests for \"replay\" or \"summarization\" of raw records for a given time period. The request \nincludes a RabbitMQ exhchange the records should be sent to.   !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/)  Running under systemd:  graccreq.service  Config in  /etc/graccreq/config.d/gracc-request.toml", 
            "title": "GRACC Request Agent"
        }, 
        {
            "location": "/ops/services/#gracc-summary-agent", 
            "text": "The Summary agent ( gracc-summary ) periodically requests summarizations, and has a CLI to manually\nsummarize periods.   !! Installed from copr (https://copr.fedorainfracloud.org/coprs/djw8605/GRACC/)  Running under systemd:   Every 15 minutes, resummarize the last 7 days:  graccsumperiodic.service  and  graccsumperiodic.timer  Every 24 hours, resummarize the last 365 days:  graccsumperiodicyearly.service  and  graccsumperiodicyearly.timer  Config in  /etc/graccsum/config.d/gracc-summary.toml", 
            "title": "GRACC Summary Agent"
        }, 
        {
            "location": "/ops/services/#apel-reporting", 
            "text": "Source at  OSG docker    docker {pull, run} opensciencegrid/gracc-apel    Running under systemd:  gracc-apel.service  and  gracc-apel.timer  Registered service certificate with  APEL admins  needed   in docker we use: $ openssl x509 -in /etc/grid-security/apel/apelcert.pem -noout -subject\n    subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=apel/hcc-grace.unl.edu  SELinux prevents inheritance of certificates, so this must be set on docker master node:  chcon -Rt svirt_sandbox_file_t /etc/grid-security/apel/", 
            "title": "APEL reporting"
        }, 
        {
            "location": "/ops/services/#configuring-gracc-apel-in-systemd", 
            "text": "gracc-apel.service  configure, enable, start, test       $ cat /lib/systemd/system/gracc-apel.service \n      [Unit]\n      Description=GRACC APEL reporting Docker container\n\n      [Service]\n      Type=oneshot\n      ExecStart=/bin/docker run -v /etc/grid-security/apel/apelcert.pem:/etc/grid-security/apel/apelcert.pem -v /etc/grid-security/apel/apelkey.pem:/etc/grid-security/apel/apelkey.pem opensciencegrid/gracc-apel\n\n    $ systemctl enabled gracc-apel.service\n    $ systemctl start gracc-apel.service\n    $ systemctl status gracc-apel.service  To run periodically:\n*  gracc-apel.timer  configure, enable, start      $ cat /lib/systemd/system/gracc-apel.timer \n    [Unit]\n    Description=Run GRACC-to-APEL reporting script\n\n    [Timer]\n    # Explicitly declare service that this timer is responsible for\n    Unit=gracc-apel.service\n    # Runs 'gracc-apel' relative to when the *timer-unit* has been activated\n    OnActiveSec=1hour\n    # Runs 'gracc-apel' relative to when *service-unit* was last deactivated\n    OnUnitInactiveSec=1hour\n\n    # Randomize runtime by a small amount each run.\n    RandomizedDelaySec=2min\n\n    [Install]\n    WantedBy=timers.target\n    $ systemctl enabled gracc-apel.timer\n    $ systemctl start gracc-apel.timer\n    $ systemctl status gracc-apel.timer  * To check if `gracc-timer.timer` runs:      $ systemctl list-timers *apel*\n    NEXT                         LEFT       LAST                         PASSED    UNIT             ACTIVATES\n    Mon 2017-05-22 17:48:37 CDT  21min left Mon 2017-05-22 16:46:40 CDT  40min ago gracc-apel.timer gracc-apel.service", 
            "title": "Configuring GRACC-APEL in systemd"
        }, 
        {
            "location": "/ops/services/#grafana", 
            "text": "Installed from official Grafana yum repo  Running under systemd:  grafana-server.service  Config in  /etc/grafana  Data in  /var/lib/grafana  Listening at:  http://localhost:3000  https://gracc.opensciencegrid.org/", 
            "title": "Grafana"
        }, 
        {
            "location": "/ops/services/#kibana", 
            "text": "Installed from official Elasticsearch yum repo  Running under systemd:  kibana.service  Config in /etc/kibana  Listening at:  http://localhost:5601  https://gracc.opensciencegrid.org/kibana/  readonlyrest  Elasticsearch plugin controls access, write actions require authentication\n  with basic auth, configured in  /etc/elasticsearch-ro/elasticsearch.yml .", 
            "title": "Kibana"
        }, 
        {
            "location": "/ops/services/#prometheus", 
            "text": "Prometheus is used for monitoring the nodes and services.   !! Installed in  /opt/prometheus  Config in  /etc/prometheus  Data in  /data/prometheus  Running under systemd:  prometheus.service  Listening at:  http://localhost:9090  https://gracc.opensciencegrid.org/prometheus   !! external access requires basic auth,  /etc/nginx/conf.d/kibana.htpasswd  limited unauthenticated access to query endpoint", 
            "title": "Prometheus"
        }, 
        {
            "location": "/ops/services/#prometheus-exporters", 
            "text": "Prometheus exporters collect data from services and present them for collection by Prometheus.", 
            "title": "Prometheus Exporters"
        }, 
        {
            "location": "/ops/services/#rabbitmq-exporter", 
            "text": "!! manually run under docker (to be moved to docker-compose)  configuration via env var  RABBIT_URL=https://event-itb.grid.iu.edu:15671  RABBIT_USER=$USER  RABBIT_PASSWORD=$PASSWORD    Listening at http://localhost:9111", 
            "title": "RabbitMQ Exporter"
        }, 
        {
            "location": "/ops/services/#node-exporter", 
            "text": "Installed at  /opt/prometheus  Run with systemd:  prometheus-node-exporter.service  Listening at http://0.0.0.0:9100", 
            "title": "Node Exporter"
        }, 
        {
            "location": "/ops/services/#graphite-exporter", 
            "text": "The Graphite exporter accepts time-series data in graphite format and exposes it for \ncollection by prometheus.   Running under docker  Listening at:  http://0.0.0.0:9100 (publish to prometheus)  https://0.0.0.0:2003 (collect from graphite)", 
            "title": "Graphite Exporter"
        }, 
        {
            "location": "/ops/services/#nginx", 
            "text": "All public services are proxied through nginx.   Installed from EPEL  Primary config in  /etc/nginx/conf.d/default.conf  Running under systemd:  nginx.service  Listening at:  http://0.0.0.0:80  http://0.0.0.0:443  http://gracc.opensciencegrid.org  https://gracc.opensciencegrid.org", 
            "title": "Nginx"
        }, 
        {
            "location": "/ops/services/#docker", 
            "text": "Running under systemd:  docker.service  Extra config in  /etc/systemd/system/docker.service.d  Container logs sent to journald; e.g.  sudo journalctl CONTAINER_NAME=graccosg_gracc-collector_1", 
            "title": "Docker"
        }, 
        {
            "location": "/ops/services/#portainer", 
            "text": "Portainer is a web-based utility for monitoring and managing containers.   running under docker  listening on http://0.0.0.0:9000  !! not proxied through nginx, need to tunnel, e.g.  ssh -L 9000:localhost:9000 gracc.opensciencegrid.org", 
            "title": "Portainer"
        }, 
        {
            "location": "/ops/services/#data-nodes-gratiav2-2-though-5", 
            "text": "Ansible is set up on the head node to perform operations across all data \nnodes, e.g.  sudo ansible elasticsearch -a 'uptime' . Configuration in /etc/ansible .", 
            "title": "Data Nodes (gratiav2-2 though -5)"
        }, 
        {
            "location": "/ops/services/#elasticsearch", 
            "text": "Installed from official elastic repository  Config in  /etc/elasticsearch  Data in   /data/elasticsearch  and  /data2/elasticsearch  Listening at http://0.0.0.0:9200 and :9300", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/ops/services/#prometheus-exporters_1", 
            "text": "", 
            "title": "Prometheus Exporters"
        }, 
        {
            "location": "/ops/services/#node-exporter_1", 
            "text": "Installed at  /opt/prometheus  Run with systemd:  prometheus-elasticsearch-exporter.service  Listening at http://0.0.0.0:9100", 
            "title": "Node Exporter"
        }, 
        {
            "location": "/ops/services/#elasticsearch-exporter", 
            "text": "Installed at  /opt/prometheus  Run with systemd:  prometheus-node-exporter.service  Listening at http://0.0.0.0:9108", 
            "title": "Elasticsearch Exporter"
        }, 
        {
            "location": "/ops/troubleshoot/", 
            "text": "Troubleshooting of GRACC services\n\n\nHelpful dashboards\n\n\n\n\nGRACC Collector Stats\n\n\nRabbitMQ queues\n\n\nProbe Record Rate\n - example for given CE\n\n\nin addition, check on \nKibana ProbeName records\n_a=(columns:!(_source),index:%27gracc.osg.raw-*%27,interval:auto,query:(query_string:(analyze_wildcard:!f,query:%27ProbeName:%22slurm:grid1.oscer.ou.edu%22%27)),sort:!(EndTime,desc)))\n\n\n\n\n\n\nOSG Connect Summary - UChicago\n\n\nSite Transfer Summary\n\n\n\n\nIssues\n\n\nSelection of issues being investigated and actions taken in order to resolve them.\n\n\nLogstash\n\n\nSymptom\n: \n\n high usage of gracc archiver memory (e.g. ~12GB)\n\n logstash seems to be backed up and not responding\n* RabbitMQ has high volume of queued messages (e.g. ~100k)\n\n\nAction\n: \n\n \nsystemctl restart elasticsearch.service\n\n\n added to \ncheck_mk\nsystemd monitoring for elasticsearch and elasticsearch-ro\n\n* for continuous high rate disconnections in RabbitMQ contact \nMarina Krenz", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/ops/troubleshoot/#troubleshooting-of-gracc-services", 
            "text": "", 
            "title": "Troubleshooting of GRACC services"
        }, 
        {
            "location": "/ops/troubleshoot/#helpful-dashboards", 
            "text": "GRACC Collector Stats  RabbitMQ queues  Probe Record Rate  - example for given CE  in addition, check on  Kibana ProbeName records _a=(columns:!(_source),index:%27gracc.osg.raw-*%27,interval:auto,query:(query_string:(analyze_wildcard:!f,query:%27ProbeName:%22slurm:grid1.oscer.ou.edu%22%27)),sort:!(EndTime,desc)))    OSG Connect Summary - UChicago  Site Transfer Summary", 
            "title": "Helpful dashboards"
        }, 
        {
            "location": "/ops/troubleshoot/#issues", 
            "text": "Selection of issues being investigated and actions taken in order to resolve them.", 
            "title": "Issues"
        }, 
        {
            "location": "/ops/troubleshoot/#logstash", 
            "text": "Symptom :   high usage of gracc archiver memory (e.g. ~12GB)  logstash seems to be backed up and not responding\n* RabbitMQ has high volume of queued messages (e.g. ~100k)  Action :    systemctl restart elasticsearch.service   added to  check_mk systemd monitoring for elasticsearch and elasticsearch-ro \n* for continuous high rate disconnections in RabbitMQ contact  Marina Krenz", 
            "title": "Logstash"
        }, 
        {
            "location": "/content/corrections/", 
            "text": "Corrections\n\n\nGRACC corrections are maintained in an Elasticsearch index (\ngracc.corrections-0\n as of this writing).\nThese are used by \ngracc-request\n during summarization to correct/normalize VO and Project names.\n\n\nThe \ngracc-correct\n tool \nprovides a simple interface to manage name corrections. It is a single Python script that requires \nthe \nelasticseach\n package (\npip install elasticsearch\n).\n\n\nCurrently GRACC does not expose a read-write interface to Elasticsearch, so the script must be run \nfrom a node within the GRACC cluster (typically the head/client node).\n\n\nNote that it might take a minute for any changes to be reflected in queries to Elasticsearch.\n\n\nUsage\n\n\ngracc-correct [-h] [--url URL] [--index INDEX]\n                                 {project,vo} {list,add,update,delete} ...\n\npositional arguments:\n  {project,vo}          Correction type\n  {list,add,update,delete}\n    list                print existing corrections\n    add                 create new correction\n    update              update existing correction\n    delete              delete existing correction(s)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --url URL             Elasticsearch URL\n  --index INDEX         Index containing corrections\n\n\n\n\nlist command\n\n\nThe \nlist\n command will fetch and print the corrections in JSON format. You can limit the number\nof results with the \n--size\n option (default 1000), or filter the results by providing a JSON document\nwith fields to match, or an arbitrary Elasticsearch query.\n\n\nExtra options:\n\n\n  --size SIZE    Max number of documents to list.\n  --query QUERY  Search query to limit results.\n  --doc DOC      Optional JSON document with correction source to match\n\n\n\n\nadd command\n\n\nThe \nadd\n command with create a new correction. You can provide a JSON document, or \n\ngracc-correct\n will prompt you for the fields. If a correction already exists with the\ngiven keys, you'll be given the option to update the correction.\n\n\nExtra options:\n\n\n  --doc DOC      Optional JSON document with correction source to match\n\n\n\n\nupdate command\n\n\nThe \nupdate\n command will update an existing correction. You can provide a JSON document, or \n\ngracc-correct\n will prompt you for the fields. If a correction doesn't already exist with\nthe given keys, you'll be given the option to create a new correction.\n\n\nExtra options:\n\n\n  --doc DOC      Optional JSON document with correction source to match\n\n\n\n\ndelete command\n\n\nThe \ndelete\n command will delete an existing correction. You can provide a JSON document, a \ncorrection ID, or \ngracc-correct\n will prompt you for the fields. It will display the matching \ncorrection, and prompt for confirmation. This will be repeated if multiple documents match.\n\n\nExtra options:\n\n\n  --doc DOC   Optional JSON document with correction source to delete.\n  --id ID     Document id to delete.\n\n\n\n\nExamples\n\n\nList VO corrections\n\n\n$ gracc-correct vo list --size 5\n18 {\nReportableVOName\n: \nnanohub\n, \nVOName\n: \n/nanohub/Role=NULL/Capability=NULL\n, \nCorrectedVOName\n: \nnanohub\n}\n19 {\nReportableVOName\n: \nops\n, \nVOName\n: \n/ops/Role=lcgadmin/Capability=NULL\n, \nCorrectedVOName\n: \nops\n}\n20 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/Role=NULL/Capability=NULL\n, \nCorrectedVOName\n: \nosg\n}\n21 {\nReportableVOName\n: \nosgedu\n, \nVOName\n: \n/osgedu/Role=NULL/Capability=NULL\n, \nCorrectedVOName\n: \nosgedu\n}\n22 {\nReportableVOName\n: \nstar\n, \nVOName\n: \n/star/Role=NULL/Capability=NULL\n, \nCorrectedVOName\n: \nstar\n}\n\n\n\n\nList Project corrections with query\n\n\n$ gracc-correct project list --query osg\n7 {\nCorrectedProjectName\n: \nOSG-Staff\n, \nProjectName\n: \nOSG-Staff\n}\n24 {\nCorrectedProjectName\n: \nOSG-STAFF\n, \nProjectName\n: \nOSG-STAFF\n}\n200 {\nCorrectedProjectName\n: \nSWC-OSG-UC14\n, \nProjectName\n: \nSWC-OSG-UC14\n}\n209 {\nCorrectedProjectName\n: \nSWC-OSG-IU15\n, \nProjectName\n: \nSWC-OSG-IU15\n}\n104 {\nCorrectedProjectName\n: \nosg\n, \nProjectName\n: \nosg\n}\n448 {\nCorrectedProjectName\n: \nOSG Staff\n, \nProjectName\n: \nOSG Staff\n}\n51 {\nCorrectedProjectName\n: \nConnectTrain\n, \nProjectName\n: \nOSG-Connect-test\n}\n52 {\nCorrectedProjectName\n: \nConnectTrain\n, \nProjectName\n: \nOSG-Connect\n}\n43 {\nCorrectedProjectName\n: \nSNOplus\n, \nProjectName\n: \nOSG-PHY00101\n}\n\n\n\n\nList VO corrections matching document\n\n\n$ gracc-correct vo list --doc '{\nReportableVOName\n:\nosg\n}' --size 5\n20 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/Role=NULL/Capability=NULL\n, \nCorrectedVOName\n: \nosg\n}\n495 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/Role=pilot/Capability=NULL\n, \nCorrectedVOName\n: \nosg\n}\n920 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/LocalGroup=external\n, \nCorrectedVOName\n: \nosg\n}\n1041 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/LocalGroup=marksant\n, \nCorrectedVOName\n: \nosg\n}\n1042 {\nReportableVOName\n: \nosg\n, \nVOName\n: \n/osg/Snowmass/Role=snowmassadmin/Capability=NULL\n, \nCorrectedVOName\n: \nosg\n}\n\n\n\n\nAdd new VO correction interactively\n\n\n$ gracc-correct vo add\nField(s) to correct:\n    VOName: example\n    ReportableVOName: example\nCorrected VOName: osg\nCorrection created. id: AVyOq_xvTIq8btIx9sGM\n\n$ gracc-correct vo list --query '_id:AVyOq_xvTIq8btIx9sGM'\nAVyOq_xvTIq8btIx9sGM {\nVOName\n: \nexample\n, \nReportableVOName\n: \nexample\n, \nCorrectedVOName\n: \nosg\n}\n\n\n\n\nAdd new VO correction with doc\n\n\n$ gracc-correct vo add --doc '{\nVOName\n:\nexample2\n,\nReportableVOName\n:\nexample\n,\nCorrectedVOName\n:\nosg\n}'\nCorrection created. id: AVyOr3nYTIq8btIx9sGN\n\n$ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN'\nAVyOr3nYTIq8btIx9sGN {\nVOName\n: \nexample2\n, \nReportableVOName\n: \nexample\n, \nCorrectedVOName\n: \nosg\n}\n\n\n\n\nUpdate VO correction interactively\n\n\n$ gracc-correct vo update\nField(s) to correct:\n    VOName: example2\n    ReportableVOName: example\nCorrected VOName: fermilab\nCorrection AVyOr3nYTIq8btIx9sGN updated.\n\n$ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN'\nAVyOr3nYTIq8btIx9sGN {\nVOName\n: \nexample2\n, \nReportableVOName\n: \nexample\n, \nCorrectedVOName\n: \nfermilab\n}\n\n\n\n\nNote: it might take a minute for the update to be reflected in the query.\n\n\nDelete correction interactively\n\n\n$ gracc-correct vo delete\nField(s) to correct:\n    VOName: example\n    ReportableVOName: example\nCorrected VOName: osg\n{u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'}\nDelete record? (Y/N) y\nCorrection AVyOq_xvTIq8btIx9sGM deleted.\n\n\n\n\nDelete correction with doc\n\n\n$ gracc-correct vo delete --doc '{\nVOName\n: \nexample2\n, \nReportableVOName\n: \nexample\n, \nCorrectedVOName\n: \nfermilab\n}'\n{u'VOName': u'example2', u'ReportableVOName': u'example', u'CorrectedVOName': u'fermilab'}\nDelete record? (Y/N) y\nCorrection AVyOr3nYTIq8btIx9sGN deleted.\n\n\n\n\nDelete correction with id\n\n\n$ gracc-correct vo delete --id AVyOwmxFTIq8btIx9sGP\n{u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'}\nDelete record? (Y/N) y\nCorrection AVyOwmxFTIq8btIx9sGP deleted.", 
            "title": "Corrections"
        }, 
        {
            "location": "/content/corrections/#corrections", 
            "text": "GRACC corrections are maintained in an Elasticsearch index ( gracc.corrections-0  as of this writing).\nThese are used by  gracc-request  during summarization to correct/normalize VO and Project names.  The  gracc-correct  tool \nprovides a simple interface to manage name corrections. It is a single Python script that requires \nthe  elasticseach  package ( pip install elasticsearch ).  Currently GRACC does not expose a read-write interface to Elasticsearch, so the script must be run \nfrom a node within the GRACC cluster (typically the head/client node).  Note that it might take a minute for any changes to be reflected in queries to Elasticsearch.", 
            "title": "Corrections"
        }, 
        {
            "location": "/content/corrections/#usage", 
            "text": "gracc-correct [-h] [--url URL] [--index INDEX]\n                                 {project,vo} {list,add,update,delete} ...\n\npositional arguments:\n  {project,vo}          Correction type\n  {list,add,update,delete}\n    list                print existing corrections\n    add                 create new correction\n    update              update existing correction\n    delete              delete existing correction(s)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --url URL             Elasticsearch URL\n  --index INDEX         Index containing corrections", 
            "title": "Usage"
        }, 
        {
            "location": "/content/corrections/#list-command", 
            "text": "The  list  command will fetch and print the corrections in JSON format. You can limit the number\nof results with the  --size  option (default 1000), or filter the results by providing a JSON document\nwith fields to match, or an arbitrary Elasticsearch query.", 
            "title": "list command"
        }, 
        {
            "location": "/content/corrections/#extra-options", 
            "text": "--size SIZE    Max number of documents to list.\n  --query QUERY  Search query to limit results.\n  --doc DOC      Optional JSON document with correction source to match", 
            "title": "Extra options:"
        }, 
        {
            "location": "/content/corrections/#add-command", 
            "text": "The  add  command with create a new correction. You can provide a JSON document, or  gracc-correct  will prompt you for the fields. If a correction already exists with the\ngiven keys, you'll be given the option to update the correction.", 
            "title": "add command"
        }, 
        {
            "location": "/content/corrections/#extra-options_1", 
            "text": "--doc DOC      Optional JSON document with correction source to match", 
            "title": "Extra options:"
        }, 
        {
            "location": "/content/corrections/#update-command", 
            "text": "The  update  command will update an existing correction. You can provide a JSON document, or  gracc-correct  will prompt you for the fields. If a correction doesn't already exist with\nthe given keys, you'll be given the option to create a new correction.", 
            "title": "update command"
        }, 
        {
            "location": "/content/corrections/#extra-options_2", 
            "text": "--doc DOC      Optional JSON document with correction source to match", 
            "title": "Extra options:"
        }, 
        {
            "location": "/content/corrections/#delete-command", 
            "text": "The  delete  command will delete an existing correction. You can provide a JSON document, a \ncorrection ID, or  gracc-correct  will prompt you for the fields. It will display the matching \ncorrection, and prompt for confirmation. This will be repeated if multiple documents match.", 
            "title": "delete command"
        }, 
        {
            "location": "/content/corrections/#extra-options_3", 
            "text": "--doc DOC   Optional JSON document with correction source to delete.\n  --id ID     Document id to delete.", 
            "title": "Extra options:"
        }, 
        {
            "location": "/content/corrections/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/content/corrections/#list-vo-corrections", 
            "text": "$ gracc-correct vo list --size 5\n18 { ReportableVOName :  nanohub ,  VOName :  /nanohub/Role=NULL/Capability=NULL ,  CorrectedVOName :  nanohub }\n19 { ReportableVOName :  ops ,  VOName :  /ops/Role=lcgadmin/Capability=NULL ,  CorrectedVOName :  ops }\n20 { ReportableVOName :  osg ,  VOName :  /osg/Role=NULL/Capability=NULL ,  CorrectedVOName :  osg }\n21 { ReportableVOName :  osgedu ,  VOName :  /osgedu/Role=NULL/Capability=NULL ,  CorrectedVOName :  osgedu }\n22 { ReportableVOName :  star ,  VOName :  /star/Role=NULL/Capability=NULL ,  CorrectedVOName :  star }", 
            "title": "List VO corrections"
        }, 
        {
            "location": "/content/corrections/#list-project-corrections-with-query", 
            "text": "$ gracc-correct project list --query osg\n7 { CorrectedProjectName :  OSG-Staff ,  ProjectName :  OSG-Staff }\n24 { CorrectedProjectName :  OSG-STAFF ,  ProjectName :  OSG-STAFF }\n200 { CorrectedProjectName :  SWC-OSG-UC14 ,  ProjectName :  SWC-OSG-UC14 }\n209 { CorrectedProjectName :  SWC-OSG-IU15 ,  ProjectName :  SWC-OSG-IU15 }\n104 { CorrectedProjectName :  osg ,  ProjectName :  osg }\n448 { CorrectedProjectName :  OSG Staff ,  ProjectName :  OSG Staff }\n51 { CorrectedProjectName :  ConnectTrain ,  ProjectName :  OSG-Connect-test }\n52 { CorrectedProjectName :  ConnectTrain ,  ProjectName :  OSG-Connect }\n43 { CorrectedProjectName :  SNOplus ,  ProjectName :  OSG-PHY00101 }", 
            "title": "List Project corrections with query"
        }, 
        {
            "location": "/content/corrections/#list-vo-corrections-matching-document", 
            "text": "$ gracc-correct vo list --doc '{ ReportableVOName : osg }' --size 5\n20 { ReportableVOName :  osg ,  VOName :  /osg/Role=NULL/Capability=NULL ,  CorrectedVOName :  osg }\n495 { ReportableVOName :  osg ,  VOName :  /osg/Role=pilot/Capability=NULL ,  CorrectedVOName :  osg }\n920 { ReportableVOName :  osg ,  VOName :  /osg/LocalGroup=external ,  CorrectedVOName :  osg }\n1041 { ReportableVOName :  osg ,  VOName :  /osg/LocalGroup=marksant ,  CorrectedVOName :  osg }\n1042 { ReportableVOName :  osg ,  VOName :  /osg/Snowmass/Role=snowmassadmin/Capability=NULL ,  CorrectedVOName :  osg }", 
            "title": "List VO corrections matching document"
        }, 
        {
            "location": "/content/corrections/#add-new-vo-correction-interactively", 
            "text": "$ gracc-correct vo add\nField(s) to correct:\n    VOName: example\n    ReportableVOName: example\nCorrected VOName: osg\nCorrection created. id: AVyOq_xvTIq8btIx9sGM\n\n$ gracc-correct vo list --query '_id:AVyOq_xvTIq8btIx9sGM'\nAVyOq_xvTIq8btIx9sGM { VOName :  example ,  ReportableVOName :  example ,  CorrectedVOName :  osg }", 
            "title": "Add new VO correction interactively"
        }, 
        {
            "location": "/content/corrections/#add-new-vo-correction-with-doc", 
            "text": "$ gracc-correct vo add --doc '{ VOName : example2 , ReportableVOName : example , CorrectedVOName : osg }'\nCorrection created. id: AVyOr3nYTIq8btIx9sGN\n\n$ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN'\nAVyOr3nYTIq8btIx9sGN { VOName :  example2 ,  ReportableVOName :  example ,  CorrectedVOName :  osg }", 
            "title": "Add new VO correction with doc"
        }, 
        {
            "location": "/content/corrections/#update-vo-correction-interactively", 
            "text": "$ gracc-correct vo update\nField(s) to correct:\n    VOName: example2\n    ReportableVOName: example\nCorrected VOName: fermilab\nCorrection AVyOr3nYTIq8btIx9sGN updated.\n\n$ gracc-correct vo list --query '_id:AVyOr3nYTIq8btIx9sGN'\nAVyOr3nYTIq8btIx9sGN { VOName :  example2 ,  ReportableVOName :  example ,  CorrectedVOName :  fermilab }  Note: it might take a minute for the update to be reflected in the query.", 
            "title": "Update VO correction interactively"
        }, 
        {
            "location": "/content/corrections/#delete-correction-interactively", 
            "text": "$ gracc-correct vo delete\nField(s) to correct:\n    VOName: example\n    ReportableVOName: example\nCorrected VOName: osg\n{u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'}\nDelete record? (Y/N) y\nCorrection AVyOq_xvTIq8btIx9sGM deleted.", 
            "title": "Delete correction interactively"
        }, 
        {
            "location": "/content/corrections/#delete-correction-with-doc", 
            "text": "$ gracc-correct vo delete --doc '{ VOName :  example2 ,  ReportableVOName :  example ,  CorrectedVOName :  fermilab }'\n{u'VOName': u'example2', u'ReportableVOName': u'example', u'CorrectedVOName': u'fermilab'}\nDelete record? (Y/N) y\nCorrection AVyOr3nYTIq8btIx9sGN deleted.", 
            "title": "Delete correction with doc"
        }, 
        {
            "location": "/content/corrections/#delete-correction-with-id", 
            "text": "$ gracc-correct vo delete --id AVyOwmxFTIq8btIx9sGP\n{u'VOName': u'example', u'ReportableVOName': u'example', u'CorrectedVOName': u'osg'}\nDelete record? (Y/N) y\nCorrection AVyOwmxFTIq8btIx9sGP deleted.", 
            "title": "Delete correction with id"
        }
    ]
}