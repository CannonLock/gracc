{
    "docs": [
        {
            "location": "/", 
            "text": "GRACC\n\n\nGRid ACcounting Collector\n\n\n(pronounced \"grok\")\n\n\n\n\nGRACC is a collection of components for implementing resource usage accounting.\n\n\nWhy GRACC?\n\n\nGRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.\n\n\nRepositories of interest:\n\n\n\n\nGRACC Collector\n.  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.\n\n\nGRACC Monitoring Emails\n.  Simple daily emails overview GRACC activity.\n\n\nGRACC Request Daemon\n. Agents that listen for replay requests from GRACC.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#gracc", 
            "text": "GRid ACcounting Collector  (pronounced \"grok\")   GRACC is a collection of components for implementing resource usage accounting.", 
            "title": "GRACC"
        }, 
        {
            "location": "/#why-gracc", 
            "text": "GRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.  Repositories of interest:   GRACC Collector .  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.  GRACC Monitoring Emails .  Simple daily emails overview GRACC activity.  GRACC Request Daemon . Agents that listen for replay requests from GRACC.", 
            "title": "Why GRACC?"
        }, 
        {
            "location": "/dev-docs/agent-arch/", 
            "text": "Agent Architecture\n\n\nThe agents that coordinate for GRACC\n\n\n\n\nUnlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.\n\n\nComponents\n\n\nThe three major centralized components of GRACC include:\n\n\n\n\nMessage queue: A \nRabbitMQ\n service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.\n\n\nGRACC\n - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.\n\n\nGRACE\n - an \nElasticSearch\n-based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.\n\n\n\n\nOther pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).\n\n\nWe also plan on developing \ngracc-replay\n, a command-line tool for initializing replay of data in the system.  This is meant to:\n\n\n\n\nUpload Gratia raw record tarballs from disk to the message queue.\n\n\nRequest raw data to be resent from a given \nGRACE\n instance to a message queue destination (likely a second \nGRACE\n instance).\n\n\nRequest summary data to be recalculated from a given \nGRACE\n instance to a message queue destination.\n\n\n\n\nAgents\n\n\n\n\nRaw Agent\n\n\nAn agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.\n\n\nSummary Agent\n\n\nThis agent has two responsibilities:\n\n\n\n\nListening to a message queue (\n/grace.\ndb\n.summary\n) for summary records.  It fetchs the records from the queue and uploads them into ElasticSearch.\n\n\n\n\nPeriodically request new summaries be made by the Listener agent.  We envision:\n\n\n\n\nEvery 15 minutes, we re-summarize the past 2 days of data.\n\n\nEvery 12 hours, we re-summarize the past 30 days of data.\n\n\n\n\nThis allows late raw records to be included in the summary information.\n\n\n\n\n\n\nListener Agent\n\n\nA agent running on \nGRACE\n.  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.\n\n\nIt listens on the known queue \n/gracc.\ndb\n.requests\n (as defined on \nMessage Queues\n).  \n\n\nFuture components\n\n\nComponents that will likely be needed in the future include:\n\n\n\n\nGRACE-B\n: Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.\n\n\nGRACE-D\n: A \ndead letter queue\n: a destination for any unparseable or otherwise-rejected records.\n\n\nSome destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a \ncondor_collector\n) and serialize it to a database.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agent-architecture", 
            "text": "The agents that coordinate for GRACC   Unlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#components", 
            "text": "The three major centralized components of GRACC include:   Message queue: A  RabbitMQ  service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.  GRACC  - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.  GRACE  - an  ElasticSearch -based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.   Other pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).  We also plan on developing  gracc-replay , a command-line tool for initializing replay of data in the system.  This is meant to:   Upload Gratia raw record tarballs from disk to the message queue.  Request raw data to be resent from a given  GRACE  instance to a message queue destination (likely a second  GRACE  instance).  Request summary data to be recalculated from a given  GRACE  instance to a message queue destination.", 
            "title": "Components"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agents", 
            "text": "", 
            "title": "Agents"
        }, 
        {
            "location": "/dev-docs/agent-arch/#raw-agent", 
            "text": "An agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.", 
            "title": "Raw Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#summary-agent", 
            "text": "This agent has two responsibilities:   Listening to a message queue ( /grace. db .summary ) for summary records.  It fetchs the records from the queue and uploads them into ElasticSearch.   Periodically request new summaries be made by the Listener agent.  We envision:   Every 15 minutes, we re-summarize the past 2 days of data.  Every 12 hours, we re-summarize the past 30 days of data.   This allows late raw records to be included in the summary information.", 
            "title": "Summary Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#listener-agent", 
            "text": "A agent running on  GRACE .  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.  It listens on the known queue  /gracc. db .requests  (as defined on  Message Queues ).", 
            "title": "Listener Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#future-components", 
            "text": "Components that will likely be needed in the future include:   GRACE-B : Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.  GRACE-D : A  dead letter queue : a destination for any unparseable or otherwise-rejected records.  Some destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a  condor_collector ) and serialize it to a database.", 
            "title": "Future components"
        }, 
        {
            "location": "/dev-docs/message-queues/", 
            "text": "Message Queues\n\n\nMessage queues used in GRACC\n\n\n\n\nIn AMQP, there is a difference between a \nqueue\n and an \nexchange\n.  Messages delivered on a \nqueue\n are read by a single subscriber; messages delivered on an \nexchange\n will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).\n\n\nWe would like collectors to serve multiple databases (hence the use of an \nexchange\n) while queues are used for messages sent to a database agent.\n\n\nWell known message queues and exchanges used:\n\n\n\n\n/gracc.\ncollector\n.raw\n - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.\n\n\n/grace.\ndb\n.summary\n - A queue that listens for summary records to insert into a specific \ndb\n.  This is used to replicate summary records from other collectors or db's.\n\n\n/grace.\ndb\n.raw\n - Raw record queue for a database instance.\n\n\n/grace.\ndb\n.requests\n - The \nAd Agent\n listens to this queue for requests for raw and summary replications.\n\n\n\n\nHere, \ndb\n is the instance name of a given database install while \ncollector\n is the instance name of an existing Gratia collector.\n\n\nThere are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:\n\n\nRaw Records\n\n\nThese are JSON-formatted documents; the key-value pairs are derived from the OGF \nUsageRecord\n format.  For ease of compatibility with the prior Gratia system, we include an \nnjobs\n attribute if a given record represents more than one job.\n\n\nThe \nRaw Records\n page has more details and the mapping from XML UsageRecord.\n\n\n{\n    \nRecordId\n: \nosg-gw-7.t2.ucsd.edu:35741.2\n,\n    \nCreateTime\n: \n2016-05-27T22:46:46Z\n,\n    \nGlobalJobId\n: \ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nLocalJobId\n: \n185777\n,\n    \nLocalUserId\n: \ncmsuser\n,\n    \nGlobalUsername\n: \ncmsuser@t2.ucsd.edu\n,\n    \nDN\n: \n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\n,\n    \nVOName\n: \n/cms/Role=production/Capability=NULL\n,\n    \nReportableVOName\n: \ncms\n,\n    \nJobName\n: \nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nMachineName\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nSubmitHost\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nStatus\n: \n0\n,\n    \nStatus_description\n: \nCondor Exit Status\n,\n    \nWallDuration\n: 617,\n    \nWallDuration_description\n: \nWas entered in seconds\n\n    \nTimeDuration\n: {\n        \nCommittedSuspensionTime\n: 0,\n        \nCommittedTime\n: 617,\n        \nCumulativeSuspensionTime\n: 0,\n        \nLocalSysCpu\n: 0,\n        \nLocalUserCpu\n: 0,\n        \nRemoteSysCpu\n: 18,\n        \nRemoteUserCpu\n: 0\n    },\n    \nCpuDuration\n: 18,\n    \nCpuDuration_system\n: 18,\n    \nCpuDuration_system_description\n: \nWas entered in seconds\n,\n    \nCpuDuration_user\n: 0,\n    \nCpuDuration_user_description\n: \nWas entered in seconds\n,\n    \nEndTime\n: \n2016-05-27T22:44:08Z\n,\n    \nStartTime\n: \n2016-05-27T22:33:51Z\n,\n    \nHost\n: \ncabinet-1-1-1.t2.ucsd.edu\n,\n    \nQueue\n: \n5\n,\n    \nQueue_description\n: \nCondor's JobUniverse field\n,\n    \nNodeCount\n: \n1\n,\n    \nNodeCount_metric\n: \nmax\n,\n    \nProcessors\n: \n1\n,\n    \nProcessors_metric\n: \nmax\n,\n    \nResource\n: {\n        \nAccountingGroup\n: \ngroup_cmsprod.cmsuser\n,\n        \nCondorMyType\n: \nJob\n,\n        \nExitBySignal\n: \nfalse\n,\n        \nExitCode\n: \n0\n,\n        \nResourceType\n: \nBatch\n,\n        \ncondor-JobStatus\n: \n4\n\n    },\n    \nNetwork\n: \n0\n,\n    \nNetwork_metric\n: \ntotal\n,\n    \nNetwork_phaseUnit\n: 617,\n    \nNetwork_storageUnit\n: \nb\n,\n    \nProbeName\n: \ncondor:osg-gw-7.t2.ucsd.edu\n,\n    \nSiteName\n: \nUCSDT2-D\n,\n    \nGrid\n: \nOSG\n,\n    \nNjobs\n: \n1\n,\n}\n\n\n\n\n\n\nNote\n\n\nWe consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).\n\n\n\n\nSummary Records\n\n\nThe summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.\n\n\nTODO: copy JSON document here\n\n\n\n\nReplay Requests\n\n\nThe replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.\n\n\nKeys\n:\n\n\n\n\nfrom\n and \nto\n: An ISO 8601 formatted date \n time string that determines the time range beginning and ending, respectively, of the data to be sent.\n\n\nkind\n: What type of records should be resent (valid values are curently \nraw\n or \nsummary\n).\n\n\ndestination\n: An exchange on the same broker where records should be sent.  Should be a string value.\n\n\nrouting_key\n: A routing key to be used when sending the data\n\n\ncontrol\n and \ncontrol_key\n: (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.\n\n\nfilter\n: (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.\n\n\n\n\nExample\n\n\n{\n  \nfrom\n: \n2016-05-10T00:00:00\n,\n  \nto\n: \n2016-05-11T00:00:00\n,\n  \nkind\n: \nraw\n,\n  \ndestination\n: \ngrace.osg.raw\n,\n  \ncontrol\n: \ncontrol-exchange\n,\n  \ncontrol_key\n: \ncontrol_routing_key\n,\n  \nfilter\n: {\n    \nquery\n: {\n      \nquery_string\n: {\n        \nquery\n: \nvo=cms\n\n      }\n    }\n  }\n}", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#message-queues", 
            "text": "Message queues used in GRACC   In AMQP, there is a difference between a  queue  and an  exchange .  Messages delivered on a  queue  are read by a single subscriber; messages delivered on an  exchange  will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).  We would like collectors to serve multiple databases (hence the use of an  exchange ) while queues are used for messages sent to a database agent.  Well known message queues and exchanges used:   /gracc. collector .raw  - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.  /grace. db .summary  - A queue that listens for summary records to insert into a specific  db .  This is used to replicate summary records from other collectors or db's.  /grace. db .raw  - Raw record queue for a database instance.  /grace. db .requests  - The  Ad Agent  listens to this queue for requests for raw and summary replications.   Here,  db  is the instance name of a given database install while  collector  is the instance name of an existing Gratia collector.  There are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#raw-records", 
            "text": "These are JSON-formatted documents; the key-value pairs are derived from the OGF  UsageRecord  format.  For ease of compatibility with the prior Gratia system, we include an  njobs  attribute if a given record represents more than one job.  The  Raw Records  page has more details and the mapping from XML UsageRecord.  {\n     RecordId :  osg-gw-7.t2.ucsd.edu:35741.2 ,\n     CreateTime :  2016-05-27T22:46:46Z ,\n     GlobalJobId :  condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     LocalJobId :  185777 ,\n     LocalUserId :  cmsuser ,\n     GlobalUsername :  cmsuser@t2.ucsd.edu ,\n     DN :  /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba ,\n     VOName :  /cms/Role=production/Capability=NULL ,\n     ReportableVOName :  cms ,\n     JobName :  osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     MachineName :  osg-gw-7.t2.ucsd.edu ,\n     SubmitHost :  osg-gw-7.t2.ucsd.edu ,\n     Status :  0 ,\n     Status_description :  Condor Exit Status ,\n     WallDuration : 617,\n     WallDuration_description :  Was entered in seconds \n     TimeDuration : {\n         CommittedSuspensionTime : 0,\n         CommittedTime : 617,\n         CumulativeSuspensionTime : 0,\n         LocalSysCpu : 0,\n         LocalUserCpu : 0,\n         RemoteSysCpu : 18,\n         RemoteUserCpu : 0\n    },\n     CpuDuration : 18,\n     CpuDuration_system : 18,\n     CpuDuration_system_description :  Was entered in seconds ,\n     CpuDuration_user : 0,\n     CpuDuration_user_description :  Was entered in seconds ,\n     EndTime :  2016-05-27T22:44:08Z ,\n     StartTime :  2016-05-27T22:33:51Z ,\n     Host :  cabinet-1-1-1.t2.ucsd.edu ,\n     Queue :  5 ,\n     Queue_description :  Condor's JobUniverse field ,\n     NodeCount :  1 ,\n     NodeCount_metric :  max ,\n     Processors :  1 ,\n     Processors_metric :  max ,\n     Resource : {\n         AccountingGroup :  group_cmsprod.cmsuser ,\n         CondorMyType :  Job ,\n         ExitBySignal :  false ,\n         ExitCode :  0 ,\n         ResourceType :  Batch ,\n         condor-JobStatus :  4 \n    },\n     Network :  0 ,\n     Network_metric :  total ,\n     Network_phaseUnit : 617,\n     Network_storageUnit :  b ,\n     ProbeName :  condor:osg-gw-7.t2.ucsd.edu ,\n     SiteName :  UCSDT2-D ,\n     Grid :  OSG ,\n     Njobs :  1 ,\n}   Note  We consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#summary-records", 
            "text": "The summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.  TODO: copy JSON document here", 
            "title": "Summary Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#replay-requests", 
            "text": "The replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.  Keys :   from  and  to : An ISO 8601 formatted date   time string that determines the time range beginning and ending, respectively, of the data to be sent.  kind : What type of records should be resent (valid values are curently  raw  or  summary ).  destination : An exchange on the same broker where records should be sent.  Should be a string value.  routing_key : A routing key to be used when sending the data  control  and  control_key : (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.  filter : (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.   Example  {\n   from :  2016-05-10T00:00:00 ,\n   to :  2016-05-11T00:00:00 ,\n   kind :  raw ,\n   destination :  grace.osg.raw ,\n   control :  control-exchange ,\n   control_key :  control_routing_key ,\n   filter : {\n     query : {\n       query_string : {\n         query :  vo=cms \n      }\n    }\n  }\n}", 
            "title": "Replay Requests"
        }, 
        {
            "location": "/dev-docs/raw-records/", 
            "text": "Raw Records\n\n\nRaw records are in JSON format with a schema derived from the \n\nOGF UsageRecord specification\n \nused by GRACC's predecessor Gratia.\n\n\nRequirements\n\n\nTo maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:\n\n\n\n\nCommonName\n\n\nVOName\n\n\nReportableVOName\n\n\nProjectName\n\n\nEndTime\n\n\nCpuDuration\n\n\nWallDuration\n\n\nProcessors\n\n\n...?\n\n\n\n\nDates and Times\n\n\nAll times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.\n\n\nConverting XML JobUsageRecord\n\n\nThe mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.\n\n\nThe raw XML record is stored in the \nRawXML\n field, to allow for later reference \nand remapping.\n\n\nIdentity Groups\n\n\nIdentity groups are flattened by moving their sub-elements to the top level:\n\n\n\n\nRecordIdentity\n\n\nRecordId\n\n\nCreateTime\n\n\n\n\n\n\nJobIdentity\n\n\nGlobalJobId\n\n\nLocalJobId\n\n\nProcessId (array)\n\n\n\n\n\n\nUserIdentity\n\n\nLocalUserId\n\n\nGlobalUsername\n\n\nCommonName\n\n\nDN\n\n\nVOName\n\n\nReportableVOName\n\n\n\n\n\n\n\n\nDurations\n\n\nDuration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:\n\n\n\n\nCpuDuration (combined)\n\n\nCpuDuration_user\n\n\nCpuDuration_system\n\n\nWallDuration\n\n\n\n\nResource\n\n\nResources are transformed into a \ndescription\n:\nvalue\n map in the Resource\nfield. The description is transformed to make it a valid field name \n(spaces and dots are converted to dashes). Other properties are flattened \ninto the Resource map as \ndescription\n_\nproperty_name\n:\nproperty_value\n.\n\n\nTimeDuration and TimeInstant elements are likewise put in \ntype\n:\nvalue\n maps \nin their respective fields. Durations are converted to seconds, discrete times \nare ISO8601 strings.\n\n\nOther\n\n\nAny other elements are directly included in the top level. Properties of those elements are moved to fields named as \nelement\n_\nproperty\n, e.g. \nJobName_description\n.\n\n\nExample XML Record\n\n\nJobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\"\n\n    \nRecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/\n\n    \nJobIdentity\n\n        \nGlobalJobId\ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/GlobalJobId\n\n        \nLocalJobId\n185777\n/LocalJobId\n\n    \n/JobIdentity\n\n    \nUserIdentity\n\n        \nLocalUserId\ncmsuser\n/LocalUserId\n\n        \nGlobalUsername\ncmsuser@t2.ucsd.edu\n/GlobalUsername\n\n        \nDN\n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\n/DN\n\n        \nVOName\n/cms/Role=production/Capability=NULL\n/VOName\n\n        \nReportableVOName\ncms\n/ReportableVOName\n\n    \n/UserIdentity\n\n    \nJobName\nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/JobName\n\n    \nMachineName\nosg-gw-7.t2.ucsd.edu\n/MachineName\n\n    \nSubmitHost\nosg-gw-7.t2.ucsd.edu\n/SubmitHost\n\n    \nStatus urwg:description=\"Condor Exit Status\"\n0\n/Status\n\n    \nWallDuration urwg:description=\"Was entered in seconds\"\nPT10M17.0S\n/WallDuration\n\n    \nTimeDuration urwg:type=\"RemoteUserCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeD\n    \nTimeDuration urwg:type=\"RemoteSysCpu\"\nPT18.0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"LocalSysCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CumulativeSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedTime\"\nPT10M17.0S\n/TimeDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\"\nPT18.0S\n/CpuDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\"\nPT0S\n/CpuDuration\n\n    \nEndTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:44:08Z\n/EndTime\n\n    \nStartTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:33:51Z\n/StartTime\n\n    \nHost primary=\"true\"\ncabinet-1-1-1.t2.ucsd.edu\n/Host\n\n    \nQueue urwg:description=\"Condor's JobUniverse field\"\n5\n/Queue\n\n    \nNodeCount urwg:metric=\"max\"\n1\n/NodeCount\n\n    \nProcessors urwg:metric=\"max\"\n1\n/Processors\n\n    \nResource urwg:description=\"CondorMyType\"\nJob\n/Resource\n\n    \nResource urwg:description=\"AccountingGroup\"\ngroup_cmsprod.cmsuser\n/Resource\n\n    \nResource urwg:description=\"ExitBySignal\"\nfalse\n/Resource\n\n    \nResource urwg:description=\"ExitCode\"\n0\n/Resource\n\n    \nResource urwg:description=\"condor.JobStatus\"\n4\n/Resource\n\n    \nNetwork urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\"\n0\n/Network\n\n    \nProbeName\ncondor:osg-gw-7.t2.ucsd.edu\n/ProbeName\n\n    \nSiteName\nUCSDT2-D\n/SiteName\n\n    \nGrid\nOSG\n/Grid\n\n    \nNjobs\n1\n/Njobs\n\n    \nResource urwg:description=\"ResourceType\"\nBatch\n/Resource\n\n\n/JobUsageRecord\n\n\n\n\nExample Corresponding JSON\n\n\n{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration\": {\n        \"CommittedSuspensionTime\": 0,\n        \"CommittedTime\": 617,\n        \"CumulativeSuspensionTime\": 0,\n        \"LocalSysCpu\": 0,\n        \"LocalUserCpu\": 0,\n        \"RemoteSysCpu\": 18,\n        \"RemoteUserCpu\": 0\n    },\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource\": {\n        \"AccountingGroup\": \"group_cmsprod.cmsuser\",\n        \"CondorMyType\": \"Job\",\n        \"ExitBySignal\": \"false\",\n        \"ExitCode\": \"0\",\n        \"ResourceType\": \"Batch\",\n        \"condor-JobStatus\": \"4\"\n    },\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#raw-records", 
            "text": "Raw records are in JSON format with a schema derived from the  OGF UsageRecord specification  \nused by GRACC's predecessor Gratia.", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#requirements", 
            "text": "To maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:   CommonName  VOName  ReportableVOName  ProjectName  EndTime  CpuDuration  WallDuration  Processors  ...?", 
            "title": "Requirements"
        }, 
        {
            "location": "/dev-docs/raw-records/#dates-and-times", 
            "text": "All times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.", 
            "title": "Dates and Times"
        }, 
        {
            "location": "/dev-docs/raw-records/#converting-xml-jobusagerecord", 
            "text": "The mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.  The raw XML record is stored in the  RawXML  field, to allow for later reference \nand remapping.", 
            "title": "Converting XML JobUsageRecord"
        }, 
        {
            "location": "/dev-docs/raw-records/#identity-groups", 
            "text": "Identity groups are flattened by moving their sub-elements to the top level:   RecordIdentity  RecordId  CreateTime    JobIdentity  GlobalJobId  LocalJobId  ProcessId (array)    UserIdentity  LocalUserId  GlobalUsername  CommonName  DN  VOName  ReportableVOName", 
            "title": "Identity Groups"
        }, 
        {
            "location": "/dev-docs/raw-records/#durations", 
            "text": "Duration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:   CpuDuration (combined)  CpuDuration_user  CpuDuration_system  WallDuration", 
            "title": "Durations"
        }, 
        {
            "location": "/dev-docs/raw-records/#resource", 
            "text": "Resources are transformed into a  description : value  map in the Resource\nfield. The description is transformed to make it a valid field name \n(spaces and dots are converted to dashes). Other properties are flattened \ninto the Resource map as  description _ property_name : property_value .  TimeDuration and TimeInstant elements are likewise put in  type : value  maps \nin their respective fields. Durations are converted to seconds, discrete times \nare ISO8601 strings.", 
            "title": "Resource"
        }, 
        {
            "location": "/dev-docs/raw-records/#other", 
            "text": "Any other elements are directly included in the top level. Properties of those elements are moved to fields named as  element _ property , e.g.  JobName_description .", 
            "title": "Other"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-xml-record", 
            "text": "JobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\" \n     RecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/ \n     JobIdentity \n         GlobalJobId condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /GlobalJobId \n         LocalJobId 185777 /LocalJobId \n     /JobIdentity \n     UserIdentity \n         LocalUserId cmsuser /LocalUserId \n         GlobalUsername cmsuser@t2.ucsd.edu /GlobalUsername \n         DN /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba /DN \n         VOName /cms/Role=production/Capability=NULL /VOName \n         ReportableVOName cms /ReportableVOName \n     /UserIdentity \n     JobName osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /JobName \n     MachineName osg-gw-7.t2.ucsd.edu /MachineName \n     SubmitHost osg-gw-7.t2.ucsd.edu /SubmitHost \n     Status urwg:description=\"Condor Exit Status\" 0 /Status \n     WallDuration urwg:description=\"Was entered in seconds\" PT10M17.0S /WallDuration \n     TimeDuration urwg:type=\"RemoteUserCpu\" PT0S /TimeDuration \n     TimeD\n     TimeDuration urwg:type=\"RemoteSysCpu\" PT18.0S /TimeDuration \n     TimeDuration urwg:type=\"LocalSysCpu\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CumulativeSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedTime\" PT10M17.0S /TimeDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\" PT18.0S /CpuDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\" PT0S /CpuDuration \n     EndTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:44:08Z /EndTime \n     StartTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:33:51Z /StartTime \n     Host primary=\"true\" cabinet-1-1-1.t2.ucsd.edu /Host \n     Queue urwg:description=\"Condor's JobUniverse field\" 5 /Queue \n     NodeCount urwg:metric=\"max\" 1 /NodeCount \n     Processors urwg:metric=\"max\" 1 /Processors \n     Resource urwg:description=\"CondorMyType\" Job /Resource \n     Resource urwg:description=\"AccountingGroup\" group_cmsprod.cmsuser /Resource \n     Resource urwg:description=\"ExitBySignal\" false /Resource \n     Resource urwg:description=\"ExitCode\" 0 /Resource \n     Resource urwg:description=\"condor.JobStatus\" 4 /Resource \n     Network urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\" 0 /Network \n     ProbeName condor:osg-gw-7.t2.ucsd.edu /ProbeName \n     SiteName UCSDT2-D /SiteName \n     Grid OSG /Grid \n     Njobs 1 /Njobs \n     Resource urwg:description=\"ResourceType\" Batch /Resource  /JobUsageRecord", 
            "title": "Example XML Record"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-corresponding-json", 
            "text": "{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration\": {\n        \"CommittedSuspensionTime\": 0,\n        \"CommittedTime\": 617,\n        \"CumulativeSuspensionTime\": 0,\n        \"LocalSysCpu\": 0,\n        \"LocalUserCpu\": 0,\n        \"RemoteSysCpu\": 18,\n        \"RemoteUserCpu\": 0\n    },\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource\": {\n        \"AccountingGroup\": \"group_cmsprod.cmsuser\",\n        \"CondorMyType\": \"Job\",\n        \"ExitBySignal\": \"false\",\n        \"ExitCode\": \"0\",\n        \"ResourceType\": \"Batch\",\n        \"condor-JobStatus\": \"4\"\n    },\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Example Corresponding JSON"
        }, 
        {
            "location": "/dev-docs/install-grace-db/", 
            "text": "Installing GRACE\n\n\nThe GRACE database service consists of:\n\n\n\n\nElasticSearch as a datastore.\n\n\ngrace-raw-listener\n: Listens to the raw records from the collector\n\n\ngrace-summary-listener\n: Listens for and requests summary records.\n\n\ngrace-request-listener\n: Listens for replay and summarization requests.\n\n\n\n\nSee the \nagent architecture docs\n for more information.\n\n\nAdditionally, for monitoring and visualization, one commonly installs\nthe following external components.\n\n\n\n\nInfluxDB (primarily for monitoring ES performance)\n\n\nGrafana (visualization)\n\n\nKibana (visualization)\n\n\n\n\nDependencies\n\n\nThis document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.\n\n\nWe also assume that the OSG repos and \nyum\n priorities have been \nappropriately configured\n.\n\n\nInstallation\n\n\nThe relevant components can be pulled in via a meta-RPM:\n\n\nyum install --enablerepo=osg-development osg-grace\n\n\n\n\nConfiguration\n\n\nConfiguration files are kept in \n/etc/gracc/config.d\n and \n/usr/share/gracc/config.d\n; files in this directory are read in lexigraphical order.  The file format is \nTOML\n.\n\n\nYou will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in \n/usr/share/gracc/config.d\n: these will be overwritten on upgrade.  Instead, start by editing the samples in \n/etc\n.\n\n\nMost strings will expand the text %I to the \"instance name\".  So, for the \nosg\n instance, the following,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n\n\n\nis equivalent to\n\n\n[AMQP]\nexchange = \ngracc.osg.raw\n\nqueue = \ngrace.osg.raw\n\n\n\n\n\nFurther, we can have specific instance overrides.  Hence,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n[AMQP.osg]\nexchange = \ngracc.osg-test.raw\n\n\n\n\n\nis equivalent to:\n\n\n[AMQP]\nexchange = \ngracc.osg-test.raw\n\nqueue = \ngracc.osg.raw\n\n\n\n\n\nRunning services\n\n\nTo configure GRACC to start at boot, you will need to do the following for the \nosg\n instance:\n\n\nln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload\n\n\n\n\nMultiple instance can be run by editing the instance name above.\n\n\nFinally, these services can be started via the typical system management commands:\n\n\nsystemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg\n\n\n\n\nLog files\n\n\nBy default, log files go into \n/var/log/gracc\n.", 
            "title": "GRACE Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installing-grace", 
            "text": "The GRACE database service consists of:   ElasticSearch as a datastore.  grace-raw-listener : Listens to the raw records from the collector  grace-summary-listener : Listens for and requests summary records.  grace-request-listener : Listens for replay and summarization requests.   See the  agent architecture docs  for more information.  Additionally, for monitoring and visualization, one commonly installs\nthe following external components.   InfluxDB (primarily for monitoring ES performance)  Grafana (visualization)  Kibana (visualization)", 
            "title": "Installing GRACE"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#dependencies", 
            "text": "This document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.  We also assume that the OSG repos and  yum  priorities have been  appropriately configured .", 
            "title": "Dependencies"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installation", 
            "text": "The relevant components can be pulled in via a meta-RPM:  yum install --enablerepo=osg-development osg-grace", 
            "title": "Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#configuration", 
            "text": "Configuration files are kept in  /etc/gracc/config.d  and  /usr/share/gracc/config.d ; files in this directory are read in lexigraphical order.  The file format is  TOML .  You will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in  /usr/share/gracc/config.d : these will be overwritten on upgrade.  Instead, start by editing the samples in  /etc .  Most strings will expand the text %I to the \"instance name\".  So, for the  osg  instance, the following,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw   is equivalent to  [AMQP]\nexchange =  gracc.osg.raw \nqueue =  grace.osg.raw   Further, we can have specific instance overrides.  Hence,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw \n\n[AMQP.osg]\nexchange =  gracc.osg-test.raw   is equivalent to:  [AMQP]\nexchange =  gracc.osg-test.raw \nqueue =  gracc.osg.raw", 
            "title": "Configuration"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#running-services", 
            "text": "To configure GRACC to start at boot, you will need to do the following for the  osg  instance:  ln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload  Multiple instance can be run by editing the instance name above.  Finally, these services can be started via the typical system management commands:  systemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg", 
            "title": "Running services"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#log-files", 
            "text": "By default, log files go into  /var/log/gracc .", 
            "title": "Log files"
        }
    ]
}