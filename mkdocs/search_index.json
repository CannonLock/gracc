{
    "docs": [
        {
            "location": "/", 
            "text": "GRACC\n\n\nGRid ACcounting Collector\n\n\n(pronounced \"grok\")\n\n\n\n\nGRACC is a collection of components for implementing resource usage accounting.\n\n\nWhy GRACC?\n\n\nGRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.\n\n\nRepositories of interest:\n\n\n\n\nGRACC Collector\n.  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.\n\n\nGRACC Monitoring Emails\n.  Simple daily emails overview GRACC activity.\n\n\nGRACC Request Daemon\n. Agents that listen for replay requests from GRACC.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#gracc", 
            "text": "GRid ACcounting Collector  (pronounced \"grok\")   GRACC is a collection of components for implementing resource usage accounting.", 
            "title": "GRACC"
        }, 
        {
            "location": "/#why-gracc", 
            "text": "GRACC is meant as a replacement of the Gratia accounting system; the engineering focus is on smaller, independent components rather than a monolithic collector architecture.  The hope is that, by breaking the functionality into a series of smaller components, future architectural changes (such as migration to a new database) can be done without rewriting the entire infrastructure.  For example, forwarding information to a separate accounting database becomes much simpler in this infrastructure.  Repositories of interest:   GRACC Collector .  An agent which runs on an existing Gratia collector that forwards raw usage records to GRACC.  GRACC Monitoring Emails .  Simple daily emails overview GRACC activity.  GRACC Request Daemon . Agents that listen for replay requests from GRACC.", 
            "title": "Why GRACC?"
        }, 
        {
            "location": "/user/standard/", 
            "text": "Standard GRACC Interfaces\n\n\nGrafana\n\n\nGrafana\n is an open-source graphing/dashboarding web application,\nwidely used for system and service monitoring. The GRACC project maintains a Grafana \ndeployment with a collection of standard dashboards at \n\nhttps://gracc.opensciencegrid.org\n.\n\n\nKibana\n\n\nWhile Grafana is great for looking at pre-defined views into the data, it's not \ngreat for exploration or ad-hoc analytics, which is the area \nKibana\n excels in. The GRACC project maintains a public read-only\nKibana interface at \n\nhttps://gracc.opensciencegrid.org/kibana\n.", 
            "title": "Standard Interfaces"
        }, 
        {
            "location": "/user/standard/#standard-gracc-interfaces", 
            "text": "", 
            "title": "Standard GRACC Interfaces"
        }, 
        {
            "location": "/user/standard/#grafana", 
            "text": "Grafana  is an open-source graphing/dashboarding web application,\nwidely used for system and service monitoring. The GRACC project maintains a Grafana \ndeployment with a collection of standard dashboards at  https://gracc.opensciencegrid.org .", 
            "title": "Grafana"
        }, 
        {
            "location": "/user/standard/#kibana", 
            "text": "While Grafana is great for looking at pre-defined views into the data, it's not \ngreat for exploration or ad-hoc analytics, which is the area  Kibana  excels in. The GRACC project maintains a public read-only\nKibana interface at  https://gracc.opensciencegrid.org/kibana .", 
            "title": "Kibana"
        }, 
        {
            "location": "/user/grafana/", 
            "text": "Accessing GRACC from Grafana\n\n\nExisting Grafana\n\n\nIf you already have a Grafana instance that you maintain, it's very easy to\nadd a GRACC datasource so you can start incorporating accounting data into\nyour dashboards.\n\n\nDatasource Settings\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\n\n\n\n\n\n\n\n\nType\n\n\nElasticsearch\n\n\n\n\n\n\nUrl\n\n\nhttps://gracc.opensciencegrid.org/q\n\n\n\n\n\n\nAccess\n\n\nProxy (either should work)\n\n\n\n\n\n\nIndex name\n\n\ngracc.osg.summary\n\n\n\n\n\n\nPattern\n\n\nno pattern\n\n\n\n\n\n\nTime field name\n\n\n@timestamp\n\n\n\n\n\n\nVersion\n\n\n5.x\n\n\n\n\n\n\nGroup by time interval\n\n\n1d", 
            "title": "Custom Grafana"
        }, 
        {
            "location": "/user/grafana/#accessing-gracc-from-grafana", 
            "text": "", 
            "title": "Accessing GRACC from Grafana"
        }, 
        {
            "location": "/user/grafana/#existing-grafana", 
            "text": "If you already have a Grafana instance that you maintain, it's very easy to\nadd a GRACC datasource so you can start incorporating accounting data into\nyour dashboards.", 
            "title": "Existing Grafana"
        }, 
        {
            "location": "/user/grafana/#datasource-settings", 
            "text": "Setting  Values      Type  Elasticsearch    Url  https://gracc.opensciencegrid.org/q    Access  Proxy (either should work)    Index name  gracc.osg.summary    Pattern  no pattern    Time field name  @timestamp    Version  5.x    Group by time interval  1d", 
            "title": "Datasource Settings"
        }, 
        {
            "location": "/user/direct/", 
            "text": "Direct Access\n\n\nGRACC runs on \nElasticsearch\n,\nwhich can be accessed via many \nclient libraries\n and tools. \n\n\nA read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q.\n\n\ncURL\n\n\nThe Elasticsearch \nquery DSL\n is quite complex, but is also very powerful.\nBelow is a relatively simple but non-trivial example of directly querying GRACC from the \ncommand-line via cURL. This query will calculate the number of payload jobs run by \nLIGO in January 2017, and the total wall time they used. \n\n\ncurl 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary '\n{\n    \nquery\n: {\n        \nquery_string\n: {\n            \nquery\n: \nVOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01]\n\n        }\n    },\n    \naggs\n: {\n        \nwalltime\n: {\n            \nsum\n: {\n                \nfield\n: \nCoreHours\n\n            }\n        },\n        \njobs\n: {\n            \nsum\n: {\n                \nfield\n: \nNjobs\n\n            }\n        }\n    },\n    \nsize\n: 0\n}'\n\n\n\n\nWhich might return:\n\n\n{\n  \ntook\n : 2,\n  \ntimed_out\n : false,\n  \n_shards\n : {\n    \ntotal\n : 6,\n    \nsuccessful\n : 6,\n    \nfailed\n : 0\n  },\n  \nhits\n : {\n    \ntotal\n : 68,\n    \nmax_score\n : 0.0,\n    \nhits\n : [ ]\n  },\n  \naggregations\n : {\n    \njobs\n : {\n      \nvalue\n : 20626.0\n    },\n    \nwalltime\n : {\n      \nvalue\n : 41323.43916666666\n    }\n  }\n}\n\n\n\n\nCompare to the \nVO Summary\n dashboard in Grafana.", 
            "title": "Direct Queries"
        }, 
        {
            "location": "/user/direct/#direct-access", 
            "text": "GRACC runs on  Elasticsearch ,\nwhich can be accessed via many  client libraries  and tools.   A read-only endpoint into the GRACC Elasticsearch is available at https://gracc.opensciencegrid.org/q.", 
            "title": "Direct Access"
        }, 
        {
            "location": "/user/direct/#curl", 
            "text": "The Elasticsearch  query DSL  is quite complex, but is also very powerful.\nBelow is a relatively simple but non-trivial example of directly querying GRACC from the \ncommand-line via cURL. This query will calculate the number of payload jobs run by \nLIGO in January 2017, and the total wall time they used.   curl 'https://gracc.opensciencegrid.org/q/gracc.osg.summary/_search?pretty' --data-binary '\n{\n     query : {\n         query_string : {\n             query :  VOName:ligo AND ResourceType:Payload AND EndTime:[2017-01-01 TO 2017-02-01] \n        }\n    },\n     aggs : {\n         walltime : {\n             sum : {\n                 field :  CoreHours \n            }\n        },\n         jobs : {\n             sum : {\n                 field :  Njobs \n            }\n        }\n    },\n     size : 0\n}'  Which might return:  {\n   took  : 2,\n   timed_out  : false,\n   _shards  : {\n     total  : 6,\n     successful  : 6,\n     failed  : 0\n  },\n   hits  : {\n     total  : 68,\n     max_score  : 0.0,\n     hits  : [ ]\n  },\n   aggregations  : {\n     jobs  : {\n       value  : 20626.0\n    },\n     walltime  : {\n       value  : 41323.43916666666\n    }\n  }\n}  Compare to the  VO Summary  dashboard in Grafana.", 
            "title": "cURL"
        }, 
        {
            "location": "/dev-docs/agent-arch/", 
            "text": "Agent Architecture\n\n\nThe agents that coordinate for GRACC\n\n\n\n\nUnlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.\n\n\nComponents\n\n\nThe three major centralized components of GRACC include:\n\n\n\n\nMessage queue: A \nRabbitMQ\n service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.\n\n\nGRACC\n - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.\n\n\nGRACE\n - an \nElasticSearch\n-based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.\n\n\n\n\nOther pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).\n\n\nWe also plan on developing \ngracc-replay\n, a command-line tool for initializing replay of data in the system.  This is meant to:\n\n\n\n\nUpload Gratia raw record tarballs from disk to the message queue.\n\n\nRequest raw data to be resent from a given \nGRACE\n instance to a message queue destination (likely a second \nGRACE\n instance).\n\n\nRequest summary data to be recalculated from a given \nGRACE\n instance to a message queue destination.\n\n\n\n\nAgents\n\n\n\n\nRaw Agent\n\n\nAn agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.\n\n\nSummary Agent\n\n\nThis agent has two responsibilities:\n\n\n\n\nListening to a message queue (\n/grace.\ndb\n.summary\n) for summary records.  It fetches the records from the queue and uploads them into ElasticSearch.  This is implemented in Logstash.\n\n\n\n\nPeriodically request new summaries be made by the Listener agent.  The current behavior of the summarizer is\n\n\n\n\nEvery 15 minutes, we re-summarize the past 7 days of data.\n\n\n\n\n\n\n\n\nThe summary agent also comes with a command line option to re-summarize larger period of times.  The command line is \ngraccsummarizer\n.  The \ngraccsummarizer\n takes a date range as arguments, further help with the command line can be found with the help option.\n\n\nListener Agent\n\n\nA agent running on \nGRACE\n.  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.\n\n\nIt listens on the known queue \n/gracc.\ndb\n.requests\n (as defined on \nMessage Queues\n).  \n\n\nIntegrate OIM Information\n\n\nThe listener agent integrates OIM information for ProjectNames.  For each summary data request it performs these operations:\n\n\n\n\nDownload the Project information in XML from a OIM URL.\n\n\nParse the XML Project information into a hash keyed by the ProjectName.\n\n\nFor each summary record, search for the project information in the OIM hashed data structure, and append the information to the record.\n\n\n\n\nThe attributes copied to the record are:\n\n\n\n\nPI Name\n - The name of the principle investigator for the project.\n\n\nOrganization\n - Institution or organization that the project belogs.\n\n\nDepartment\n - Department inside the institution.\n\n\nField Of Science\n - Field of science inside the Organization.\n\n\n\n\nWe currently do not support the addition of \"Sponsor Campus Grids\".\n\n\nFuture components\n\n\nComponents that will likely be needed in the future include:\n\n\n\n\nGRACE-B\n: Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.\n\n\nGRACE-D\n: A \ndead letter queue\n: a destination for any unparseable or otherwise-rejected records.\n\n\nSome destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a \ncondor_collector\n) and serialize it to a database.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agent-architecture", 
            "text": "The agents that coordinate for GRACC   Unlike its predecessor Gratia, GRACC is split into a number of agents that coordinate through a message queue.  The intent is that this separates the distinct components into separate modules that can evolve at independent rates.  Further, it provides a mechanism for external entities interested in accounting data to integrate into the system.", 
            "title": "Agent Architecture"
        }, 
        {
            "location": "/dev-docs/agent-arch/#components", 
            "text": "The three major centralized components of GRACC include:   Message queue: A  RabbitMQ  service for exchanging messages between system components.  Utilized for its publish-subscribe model and its standardized wire format.  GRACC  - a centralized collector endpoint.  This is a HTTP-based service that listens for incoming records from legacy probes or  Gratia collectors and sends them to the message queue.  GRACE  - an  ElasticSearch -based data storage service.  Consists of an ElasticSearch database instance and several agents used to populate the system.   Other pieces of the accounting infrastructure include the site probes (which produce the records) and planned web views of the accounting data (likely based on Grafana or Kibana).  We also plan on developing  gracc-replay , a command-line tool for initializing replay of data in the system.  This is meant to:   Upload Gratia raw record tarballs from disk to the message queue.  Request raw data to be resent from a given  GRACE  instance to a message queue destination (likely a second  GRACE  instance).  Request summary data to be recalculated from a given  GRACE  instance to a message queue destination.", 
            "title": "Components"
        }, 
        {
            "location": "/dev-docs/agent-arch/#agents", 
            "text": "", 
            "title": "Agents"
        }, 
        {
            "location": "/dev-docs/agent-arch/#raw-agent", 
            "text": "An agent which listens to one or more message queues (typically, its own queue for replay information and one or more collector queues) for raw records.  Records are read off the queue and uploaded to the database.", 
            "title": "Raw Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#summary-agent", 
            "text": "This agent has two responsibilities:   Listening to a message queue ( /grace. db .summary ) for summary records.  It fetches the records from the queue and uploads them into ElasticSearch.  This is implemented in Logstash.   Periodically request new summaries be made by the Listener agent.  The current behavior of the summarizer is   Every 15 minutes, we re-summarize the past 7 days of data.     The summary agent also comes with a command line option to re-summarize larger period of times.  The command line is  graccsummarizer .  The  graccsummarizer  takes a date range as arguments, further help with the command line can be found with the help option.", 
            "title": "Summary Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#listener-agent", 
            "text": "A agent running on  GRACE .  The listener agent listens for one-time data replication requests (for either raw or summary data) on the message queue and launches an appropriate sub-process to send the data to the requested destination.  It listens on the known queue  /gracc. db .requests  (as defined on  Message Queues ).", 
            "title": "Listener Agent"
        }, 
        {
            "location": "/dev-docs/agent-arch/#integrate-oim-information", 
            "text": "The listener agent integrates OIM information for ProjectNames.  For each summary data request it performs these operations:   Download the Project information in XML from a OIM URL.  Parse the XML Project information into a hash keyed by the ProjectName.  For each summary record, search for the project information in the OIM hashed data structure, and append the information to the record.   The attributes copied to the record are:   PI Name  - The name of the principle investigator for the project.  Organization  - Institution or organization that the project belogs.  Department  - Department inside the institution.  Field Of Science  - Field of science inside the Organization.   We currently do not support the addition of \"Sponsor Campus Grids\".", 
            "title": "Integrate OIM Information"
        }, 
        {
            "location": "/dev-docs/agent-arch/#future-components", 
            "text": "Components that will likely be needed in the future include:   GRACE-B : Listens for raw records and serializes them to disk; on a daily basis, compact them into a tarball and upload them to archival storage.  GRACE-D : A  dead letter queue : a destination for any unparseable or otherwise-rejected records.  Some destination for status information.  Every 15 minutes, each component should generate a short status update (analogous to a HTCondor daemon's ClassAd in a  condor_collector ) and serialize it to a database.", 
            "title": "Future components"
        }, 
        {
            "location": "/dev-docs/message-queues/", 
            "text": "Message Queues\n\n\nMessage queues used in GRACC\n\n\n\n\nIn AMQP, there is a difference between a \nqueue\n and an \nexchange\n.  Messages delivered on a \nqueue\n are read by a single subscriber; messages delivered on an \nexchange\n will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).\n\n\nWe would like collectors to serve multiple databases (hence the use of an \nexchange\n) while queues are used for messages sent to a database agent.\n\n\nWell known message queues and exchanges used:\n\n\n\n\n/gracc.\ncollector\n.raw\n - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.\n\n\n/grace.\ndb\n.summary\n - A queue that listens for summary records to insert into a specific \ndb\n.  This is used to replicate summary records from other collectors or db's.\n\n\n/grace.\ndb\n.raw\n - Raw record queue for a database instance.\n\n\n/grace.\ndb\n.requests\n - The \nAd Agent\n listens to this queue for requests for raw and summary replications.\n\n\n\n\nHere, \ndb\n is the instance name of a given database install while \ncollector\n is the instance name of an existing Gratia collector.\n\n\nThere are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:\n\n\nRaw Records\n\n\nThese are JSON-formatted documents; the key-value pairs are derived from the OGF \nUsageRecord\n format.  For ease of compatibility with the prior Gratia system, we include an \nnjobs\n attribute if a given record represents more than one job.\n\n\nThe \nRaw Records\n page has more details and the mapping from XML UsageRecord.\n\n\n{\n    \nRecordId\n: \nosg-gw-7.t2.ucsd.edu:35741.2\n,\n    \nCreateTime\n: \n2016-05-27T22:46:46Z\n,\n    \nGlobalJobId\n: \ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nLocalJobId\n: \n185777\n,\n    \nLocalUserId\n: \ncmsuser\n,\n    \nGlobalUsername\n: \ncmsuser@t2.ucsd.edu\n,\n    \nDN\n: \n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User\n,\n    \nVOName\n: \n/cms/Role=production/Capability=NULL\n,\n    \nReportableVOName\n: \ncms\n,\n    \nJobName\n: \nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n,\n    \nMachineName\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nSubmitHost\n: \nosg-gw-7.t2.ucsd.edu\n,\n    \nStatus\n: \n0\n,\n    \nStatus_description\n: \nCondor Exit Status\n,\n    \nWallDuration\n: 617,\n    \nCpuDuration\n: 18,\n    \nCpuDuration_system\n: 18,\n    \nCpuDuration_user\n: 0,\n    \nEndTime\n: \n2016-05-27T22:44:08Z\n,\n    \nStartTime\n: \n2016-05-27T22:33:51Z\n,\n    \nHost\n: \ncabinet-1-1-1.t2.ucsd.edu\n,\n    \nNodeCount\n: \n1\n,\n    \nNodeCount_metric\n: \nmax\n,\n    \nProcessors\n: \n1\n,\n    \nProcessors_metric\n: \nmax\n,\n    \nResourceType\n: \nBatch\n,\n    \nProbeName\n: \ncondor:osg-gw-7.t2.ucsd.edu\n,\n    \nSiteName\n: \nUCSDT2-D\n,\n    \nGrid\n: \nOSG\n,\n    \nNjobs\n: \n1\n,\n}\n\n\n\n\n\n\nNote\n\n\nWe consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).\n\n\n\n\nSummary Records\n\n\nThe summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.\n\n\nTODO: copy JSON document here\n\n\n\n\nReplay Requests\n\n\nThe replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.\n\n\nKeys\n:\n\n\n\n\nfrom\n and \nto\n: An ISO 8601 formatted date \n time string that determines the time range beginning and ending, respectively, of the data to be sent.\n\n\nkind\n: What type of records should be resent (valid values are curently \nraw\n or \nsummary\n).\n\n\ndestination\n: An exchange on the same broker where records should be sent.  Should be a string value.\n\n\nrouting_key\n: A routing key to be used when sending the data\n\n\ncontrol\n and \ncontrol_key\n: (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.\n\n\nfilter\n: (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.\n\n\n\n\nExample\n\n\n{\n  \nfrom\n: \n2016-05-10T00:00:00\n,\n  \nto\n: \n2016-05-11T00:00:00\n,\n  \nkind\n: \nraw\n,\n  \ndestination\n: \ngrace.osg.raw\n,\n  \nrouting_key\n: \ngrace.osg.raw\n,\n  \ncontrol\n: \ncontrol-exchange\n,\n  \ncontrol_key\n: \ncontrol_routing_key\n,\n  \nfilter\n: {\n    \nquery\n: {\n      \nquery_string\n: {\n        \nquery\n: \nvo=cms\n\n      }\n    }\n  }\n}", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#message-queues", 
            "text": "Message queues used in GRACC   In AMQP, there is a difference between a  queue  and an  exchange .  Messages delivered on a  queue  are read by a single subscriber; messages delivered on an  exchange  will be delivered to all subscribers (implying they may be buffered for some time at the broker if a given client goes missing).  We would like collectors to serve multiple databases (hence the use of an  exchange ) while queues are used for messages sent to a database agent.  Well known message queues and exchanges used:   /gracc. collector .raw  - An exchange which listens to raw records to insert into the collector.  This is the interface that probes would send raw records.  /grace. db .summary  - A queue that listens for summary records to insert into a specific  db .  This is used to replicate summary records from other collectors or db's.  /grace. db .raw  - Raw record queue for a database instance.  /grace. db .requests  - The  Ad Agent  listens to this queue for requests for raw and summary replications.   Here,  db  is the instance name of a given database install while  collector  is the instance name of an existing Gratia collector.  There are currently three defined message schemas in GRACC: raw records, summary records, and replay requests:", 
            "title": "Message Queues"
        }, 
        {
            "location": "/dev-docs/message-queues/#raw-records", 
            "text": "These are JSON-formatted documents; the key-value pairs are derived from the OGF  UsageRecord  format.  For ease of compatibility with the prior Gratia system, we include an  njobs  attribute if a given record represents more than one job.  The  Raw Records  page has more details and the mapping from XML UsageRecord.  {\n     RecordId :  osg-gw-7.t2.ucsd.edu:35741.2 ,\n     CreateTime :  2016-05-27T22:46:46Z ,\n     GlobalJobId :  condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     LocalJobId :  185777 ,\n     LocalUserId :  cmsuser ,\n     GlobalUsername :  cmsuser@t2.ucsd.edu ,\n     DN :  /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=cmsuser/CN=1234567/CN=CMS User ,\n     VOName :  /cms/Role=production/Capability=NULL ,\n     ReportableVOName :  cms ,\n     JobName :  osg-gw-7.t2.ucsd.edu#185777.0#1464388242 ,\n     MachineName :  osg-gw-7.t2.ucsd.edu ,\n     SubmitHost :  osg-gw-7.t2.ucsd.edu ,\n     Status :  0 ,\n     Status_description :  Condor Exit Status ,\n     WallDuration : 617,\n     CpuDuration : 18,\n     CpuDuration_system : 18,\n     CpuDuration_user : 0,\n     EndTime :  2016-05-27T22:44:08Z ,\n     StartTime :  2016-05-27T22:33:51Z ,\n     Host :  cabinet-1-1-1.t2.ucsd.edu ,\n     NodeCount :  1 ,\n     NodeCount_metric :  max ,\n     Processors :  1 ,\n     Processors_metric :  max ,\n     ResourceType :  Batch ,\n     ProbeName :  condor:osg-gw-7.t2.ucsd.edu ,\n     SiteName :  UCSDT2-D ,\n     Grid :  OSG ,\n     Njobs :  1 ,\n}   Note  We consider these to be \"base\" keys: additional ones may be given (for example, if the record is derived from a HTCondor ClassAd).", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#summary-records", 
            "text": "The summary record represents a grouping of multiple similar raw records.  In GRACC, we often group jobs run on the same date, by the same user, on the same resource.  TODO: copy JSON document here", 
            "title": "Summary Records"
        }, 
        {
            "location": "/dev-docs/message-queues/#replay-requests", 
            "text": "The replay request indicates that a remote listener agent attached to an ElasticSearch database should load and re-send some amount of data.  Keys :   from  and  to : An ISO 8601 formatted date   time string that determines the time range beginning and ending, respectively, of the data to be sent.  kind : What type of records should be resent (valid values are curently  raw  or  summary ).  destination : An exchange on the same broker where records should be sent.  Should be a string value.  routing_key : A routing key to be used when sending the data  control  and  control_key : (optional) Control channel that will be notified when the data stream starts and ends.  Further, it will receive any errors that may occur during the replay.  filter : (not implemented) A ElasticSearch-formatted query filter (JSON value).  Only records matching this filter should be sent.   Example  {\n   from :  2016-05-10T00:00:00 ,\n   to :  2016-05-11T00:00:00 ,\n   kind :  raw ,\n   destination :  grace.osg.raw ,\n   routing_key :  grace.osg.raw ,\n   control :  control-exchange ,\n   control_key :  control_routing_key ,\n   filter : {\n     query : {\n       query_string : {\n         query :  vo=cms \n      }\n    }\n  }\n}", 
            "title": "Replay Requests"
        }, 
        {
            "location": "/dev-docs/raw-records/", 
            "text": "Raw Records\n\n\nRaw records are in JSON format with a schema derived from the \n\nOGF UsageRecord specification\n \nused by GRACC's predecessor Gratia.\n\n\nRequirements\n\n\nTo maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:\n\n\n\n\nResourceType\n\n\nCommonName\n\n\nVOName\n\n\nReportableVOName\n\n\nProjectName\n\n\nEndTime\n\n\nCpuDuration\n\n\nWallDuration\n\n\nProcessors\n\n\n...?\n\n\n\n\nDates and Times\n\n\nAll times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.\n\n\nConverting XML JobUsageRecord\n\n\nThe mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.\n\n\nThe raw XML record is stored in the \nRawXML\n field, to allow for later reference \nand remapping.\n\n\nIdentity Groups\n\n\nIdentity groups are flattened by moving their sub-elements to the top level:\n\n\n\n\nRecordIdentity\n\n\nRecordId\n\n\nCreateTime\n\n\nJobIdentity\n\n\nGlobalJobId\n\n\nLocalJobId\n\n\nProcessId (array)\n\n\nUserIdentity\n\n\nLocalUserId\n\n\nGlobalUsername\n\n\nCommonName\n\n\nDN\n\n\nVOName\n\n\nReportableVOName\n\n\n\n\nDurations\n\n\nDuration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:\n\n\n\n\nCpuDuration (combined)\n\n\nCpuDuration_user\n\n\nCpuDuration_system\n\n\nWallDuration\n\n\n\n\nResource\n\n\nResources are transformed into a \ndescription\n:\nvalue\n map.\nThe description is transformed to make it a valid field name \n(spaces and dots are converted to dashes) prefixed with \nResource_\n. \nOther properties are flattened as \nResource_\ndescription\n_\nproperty_name\n:\nproperty_value\n.\n\n\nThe special Resource \nResourceType\n is moved directly to the top level. ResourceType \n\nBatchPilot\n is renamed \nPayload\n due to the former being misleading.\n\n\nTimeDuration and TimeInstant elements are likewise put in \ntype\n:\nvalue\n maps \nwith their respective prefixes. Durations are converted to seconds, discrete times \nare ISO8601 strings.\n\n\nOther\n\n\nAny other elements are directly included in the top level. Properties of those elements are moved to fields named as \nelement\n_\nproperty\n, e.g. \nJobName_description\n.\n\n\nExample XML Record\n\n\nJobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\"\n\n    \nRecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/\n\n    \nJobIdentity\n\n        \nGlobalJobId\ncondor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/GlobalJobId\n\n        \nLocalJobId\n185777\n/LocalJobId\n\n    \n/JobIdentity\n\n    \nUserIdentity\n\n        \nLocalUserId\ncmsuser\n/LocalUserId\n\n        \nGlobalUsername\ncmsuser@t2.ucsd.edu\n/GlobalUsername\n\n        \nDN\n/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\n/DN\n\n        \nVOName\n/cms/Role=production/Capability=NULL\n/VOName\n\n        \nReportableVOName\ncms\n/ReportableVOName\n\n    \n/UserIdentity\n\n    \nJobName\nosg-gw-7.t2.ucsd.edu#185777.0#1464388242\n/JobName\n\n    \nMachineName\nosg-gw-7.t2.ucsd.edu\n/MachineName\n\n    \nSubmitHost\nosg-gw-7.t2.ucsd.edu\n/SubmitHost\n\n    \nStatus urwg:description=\"Condor Exit Status\"\n0\n/Status\n\n    \nWallDuration urwg:description=\"Was entered in seconds\"\nPT10M17.0S\n/WallDuration\n\n    \nTimeDuration urwg:type=\"RemoteUserCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeD\n    \nTimeDuration urwg:type=\"RemoteSysCpu\"\nPT18.0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"LocalSysCpu\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CumulativeSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedSuspensionTime\"\nPT0S\n/TimeDuration\n\n    \nTimeDuration urwg:type=\"CommittedTime\"\nPT10M17.0S\n/TimeDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\"\nPT18.0S\n/CpuDuration\n\n    \nCpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\"\nPT0S\n/CpuDuration\n\n    \nEndTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:44:08Z\n/EndTime\n\n    \nStartTime urwg:description=\"Was entered in seconds\"\n2016-05-27T22:33:51Z\n/StartTime\n\n    \nHost primary=\"true\"\ncabinet-1-1-1.t2.ucsd.edu\n/Host\n\n    \nQueue urwg:description=\"Condor's JobUniverse field\"\n5\n/Queue\n\n    \nNodeCount urwg:metric=\"max\"\n1\n/NodeCount\n\n    \nProcessors urwg:metric=\"max\"\n1\n/Processors\n\n    \nResource urwg:description=\"CondorMyType\"\nJob\n/Resource\n\n    \nResource urwg:description=\"AccountingGroup\"\ngroup_cmsprod.cmsuser\n/Resource\n\n    \nResource urwg:description=\"ExitBySignal\"\nfalse\n/Resource\n\n    \nResource urwg:description=\"ExitCode\"\n0\n/Resource\n\n    \nResource urwg:description=\"condor.JobStatus\"\n4\n/Resource\n\n    \nNetwork urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\"\n0\n/Network\n\n    \nProbeName\ncondor:osg-gw-7.t2.ucsd.edu\n/ProbeName\n\n    \nSiteName\nUCSDT2-D\n/SiteName\n\n    \nGrid\nOSG\n/Grid\n\n    \nNjobs\n1\n/Njobs\n\n    \nResource urwg:description=\"ResourceType\"\nBatch\n/Resource\n\n\n/JobUsageRecord\n\n\n\n\nExample Corresponding JSON\n\n\n{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration_CommittedSuspensionTime\": 0,\n    \"TimeDuration_CommittedTime\": 617,\n    \"TimeDuration_CumulativeSuspensionTime\": 0,\n    \"TimeDuration_LocalSysCpu\": 0,\n    \"TimeDuration_LocalUserCpu\": 0,\n    \"TimeDuration_RemoteSysCpu\": 18,\n    \"TimeDuration_RemoteUserCpu\": 0,\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\",\n    \"Resource_CondorMyType\": \"Job\",\n    \"Resource_ExitBySignal\": \"false\",\n    \"Resource_ExitCode\": \"0\",\n    \"Resource_condor-JobStatus\": \"4\",\n    \"ResourceType\": \"Batch\",\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#raw-records", 
            "text": "Raw records are in JSON format with a schema derived from the  OGF UsageRecord specification  \nused by GRACC's predecessor Gratia.", 
            "title": "Raw Records"
        }, 
        {
            "location": "/dev-docs/raw-records/#requirements", 
            "text": "To maintain flexibility and fully leverage the schemaless storage being used,\nthe schema requirements are kept to a minimum. Some fields are expected by \nGRACC compenents so leaving them out will result in the records not being\nproperly accounted:   ResourceType  CommonName  VOName  ReportableVOName  ProjectName  EndTime  CpuDuration  WallDuration  Processors  ...?", 
            "title": "Requirements"
        }, 
        {
            "location": "/dev-docs/raw-records/#dates-and-times", 
            "text": "All times are strings in ISO8601 format. Time durations are floats \nrepresenting seconds.", 
            "title": "Dates and Times"
        }, 
        {
            "location": "/dev-docs/raw-records/#converting-xml-jobusagerecord", 
            "text": "The mapping from an XML JobUsageRecord to a JSON Raw GRACC record is\noutlined below; this should help inform how new records are generated as well.  The raw XML record is stored in the  RawXML  field, to allow for later reference \nand remapping.", 
            "title": "Converting XML JobUsageRecord"
        }, 
        {
            "location": "/dev-docs/raw-records/#identity-groups", 
            "text": "Identity groups are flattened by moving their sub-elements to the top level:   RecordIdentity  RecordId  CreateTime  JobIdentity  GlobalJobId  LocalJobId  ProcessId (array)  UserIdentity  LocalUserId  GlobalUsername  CommonName  DN  VOName  ReportableVOName", 
            "title": "Identity Groups"
        }, 
        {
            "location": "/dev-docs/raw-records/#durations", 
            "text": "Duration fields are converted to seconds.\nCpuDuration can have usage \"user\" or \"system\", these are also moved into\nthe top level:   CpuDuration (combined)  CpuDuration_user  CpuDuration_system  WallDuration", 
            "title": "Durations"
        }, 
        {
            "location": "/dev-docs/raw-records/#resource", 
            "text": "Resources are transformed into a  description : value  map.\nThe description is transformed to make it a valid field name \n(spaces and dots are converted to dashes) prefixed with  Resource_ . \nOther properties are flattened as  Resource_ description _ property_name : property_value .  The special Resource  ResourceType  is moved directly to the top level. ResourceType  BatchPilot  is renamed  Payload  due to the former being misleading.  TimeDuration and TimeInstant elements are likewise put in  type : value  maps \nwith their respective prefixes. Durations are converted to seconds, discrete times \nare ISO8601 strings.", 
            "title": "Resource"
        }, 
        {
            "location": "/dev-docs/raw-records/#other", 
            "text": "Any other elements are directly included in the top level. Properties of those elements are moved to fields named as  element _ property , e.g.  JobName_description .", 
            "title": "Other"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-xml-record", 
            "text": "JobUsageRecord xmlns=\"http://www.gridforum.org/2003/ur-wg\" xmlns:urwg=\"http://www.gridforum.org/2003/ur-wg\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.gridforum.org/2003/ur-wg file:///u:/OSG/urwg-schema.11.xsd\" \n     RecordIdentity urwg:createTime=\"2016-05-27T22:46:46Z\" urwg:recordId=\"osg-gw-7.t2.ucsd.edu:35741.2\"/ \n     JobIdentity \n         GlobalJobId condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /GlobalJobId \n         LocalJobId 185777 /LocalJobId \n     /JobIdentity \n     UserIdentity \n         LocalUserId cmsuser /LocalUserId \n         GlobalUsername cmsuser@t2.ucsd.edu /GlobalUsername \n         DN /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba /DN \n         VOName /cms/Role=production/Capability=NULL /VOName \n         ReportableVOName cms /ReportableVOName \n     /UserIdentity \n     JobName osg-gw-7.t2.ucsd.edu#185777.0#1464388242 /JobName \n     MachineName osg-gw-7.t2.ucsd.edu /MachineName \n     SubmitHost osg-gw-7.t2.ucsd.edu /SubmitHost \n     Status urwg:description=\"Condor Exit Status\" 0 /Status \n     WallDuration urwg:description=\"Was entered in seconds\" PT10M17.0S /WallDuration \n     TimeDuration urwg:type=\"RemoteUserCpu\" PT0S /TimeDuration \n     TimeD\n     TimeDuration urwg:type=\"RemoteSysCpu\" PT18.0S /TimeDuration \n     TimeDuration urwg:type=\"LocalSysCpu\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CumulativeSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedSuspensionTime\" PT0S /TimeDuration \n     TimeDuration urwg:type=\"CommittedTime\" PT10M17.0S /TimeDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"system\" PT18.0S /CpuDuration \n     CpuDuration urwg:description=\"Was entered in seconds\" urwg:usageType=\"user\" PT0S /CpuDuration \n     EndTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:44:08Z /EndTime \n     StartTime urwg:description=\"Was entered in seconds\" 2016-05-27T22:33:51Z /StartTime \n     Host primary=\"true\" cabinet-1-1-1.t2.ucsd.edu /Host \n     Queue urwg:description=\"Condor's JobUniverse field\" 5 /Queue \n     NodeCount urwg:metric=\"max\" 1 /NodeCount \n     Processors urwg:metric=\"max\" 1 /Processors \n     Resource urwg:description=\"CondorMyType\" Job /Resource \n     Resource urwg:description=\"AccountingGroup\" group_cmsprod.cmsuser /Resource \n     Resource urwg:description=\"ExitBySignal\" false /Resource \n     Resource urwg:description=\"ExitCode\" 0 /Resource \n     Resource urwg:description=\"condor.JobStatus\" 4 /Resource \n     Network urwg:metric=\"total\" urwg:phaseUnit=\"PT10M17.0S\" urwg:storageUnit=\"b\" 0 /Network \n     ProbeName condor:osg-gw-7.t2.ucsd.edu /ProbeName \n     SiteName UCSDT2-D /SiteName \n     Grid OSG /Grid \n     Njobs 1 /Njobs \n     Resource urwg:description=\"ResourceType\" Batch /Resource  /JobUsageRecord", 
            "title": "Example XML Record"
        }, 
        {
            "location": "/dev-docs/raw-records/#example-corresponding-json", 
            "text": "{\n    \"RecordId\": \"osg-gw-7.t2.ucsd.edu:35741.2\",\n    \"CreateTime\": \"2016-05-27T22:46:46Z\",\n    \"GlobalJobId\": \"condor.osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"LocalJobId\": \"185777\",\n    \"LocalUserId\": \"cmsuser\",\n    \"GlobalUsername\": \"cmsuser@t2.ucsd.edu\",\n    \"DN\": \"/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=sciaba/CN=430796/CN=Andrea Sciaba\",\n    \"VOName\": \"/cms/Role=production/Capability=NULL\",\n    \"ReportableVOName\": \"cms\",\n    \"JobName\": \"osg-gw-7.t2.ucsd.edu#185777.0#1464388242\",\n    \"MachineName\": \"osg-gw-7.t2.ucsd.edu\",\n    \"SubmitHost\": \"osg-gw-7.t2.ucsd.edu\",\n    \"Status\": \"0\",\n    \"Status_description\": \"Condor Exit Status\",\n    \"WallDuration\": 617,\n    \"WallDuration_description\": \"Was entered in seconds\"\n    \"TimeDuration_CommittedSuspensionTime\": 0,\n    \"TimeDuration_CommittedTime\": 617,\n    \"TimeDuration_CumulativeSuspensionTime\": 0,\n    \"TimeDuration_LocalSysCpu\": 0,\n    \"TimeDuration_LocalUserCpu\": 0,\n    \"TimeDuration_RemoteSysCpu\": 18,\n    \"TimeDuration_RemoteUserCpu\": 0,\n    \"CpuDuration\": 18,\n    \"CpuDuration_system\": 18,\n    \"CpuDuration_system_description\": \"Was entered in seconds\",\n    \"CpuDuration_user\": 0,\n    \"CpuDuration_user_description\": \"Was entered in seconds\",\n    \"EndTime\": \"2016-05-27T22:44:08Z\",\n    \"StartTime\": \"2016-05-27T22:33:51Z\",\n    \"Host\": \"cabinet-1-1-1.t2.ucsd.edu\",\n    \"Queue\": \"5\",\n    \"Queue_description\": \"Condor's JobUniverse field\",\n    \"NodeCount\": \"1\",\n    \"NodeCount_metric\": \"max\",\n    \"Processors\": \"1\",\n    \"Processors_metric\": \"max\",\n    \"Resource_AccountingGroup\": \"group_cmsprod.cmsuser\",\n    \"Resource_CondorMyType\": \"Job\",\n    \"Resource_ExitBySignal\": \"false\",\n    \"Resource_ExitCode\": \"0\",\n    \"Resource_condor-JobStatus\": \"4\",\n    \"ResourceType\": \"Batch\",\n    \"Network\": \"0\",\n    \"Network_metric\": \"total\",\n    \"Network_phaseUnit\": 617,\n    \"Network_storageUnit\": \"b\",\n    \"ProbeName\": \"condor:osg-gw-7.t2.ucsd.edu\",\n    \"SiteName\": \"UCSDT2-D\",\n    \"Grid\": \"OSG\",\n    \"Njobs\": \"1\",\n}", 
            "title": "Example Corresponding JSON"
        }, 
        {
            "location": "/dev-docs/install-grace-db/", 
            "text": "Installing GRACE\n\n\nThe GRACE database service consists of:\n\n\n\n\nElasticSearch as a datastore.\n\n\ngrace-raw-listener\n: Listens to the raw records from the collector\n\n\ngrace-summary-listener\n: Listens for and requests summary records.\n\n\ngrace-request-listener\n: Listens for replay and summarization requests.\n\n\n\n\nSee the \nagent architecture docs\n for more information.\n\n\nAdditionally, for monitoring and visualization, one commonly installs\nthe following external components.\n\n\n\n\nInfluxDB (primarily for monitoring ES performance)\n\n\nGrafana (visualization)\n\n\nKibana (visualization)\n\n\n\n\nDependencies\n\n\nThis document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.\n\n\nWe also assume that the OSG repos and \nyum\n priorities have been \nappropriately configured\n.\n\n\nInstallation\n\n\nThe relevant components can be pulled in via a meta-RPM:\n\n\nyum install --enablerepo=osg-development osg-grace\n\n\n\n\nConfiguration\n\n\nConfiguration files are kept in \n/etc/gracc/config.d\n and \n/usr/share/gracc/config.d\n; files in this directory are read in lexigraphical order.  The file format is \nTOML\n.\n\n\nYou will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in \n/usr/share/gracc/config.d\n: these will be overwritten on upgrade.  Instead, start by editing the samples in \n/etc\n.\n\n\nMost strings will expand the text %I to the \"instance name\".  So, for the \nosg\n instance, the following,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n\n\n\nis equivalent to\n\n\n[AMQP]\nexchange = \ngracc.osg.raw\n\nqueue = \ngrace.osg.raw\n\n\n\n\n\nFurther, we can have specific instance overrides.  Hence,\n\n\n[AMQP]\nexchange = \ngracc.%I.raw\n\nqueue = \ngrace.%I.raw\n\n\n[AMQP.osg]\nexchange = \ngracc.osg-test.raw\n\n\n\n\n\nis equivalent to:\n\n\n[AMQP]\nexchange = \ngracc.osg-test.raw\n\nqueue = \ngracc.osg.raw\n\n\n\n\n\nRunning services\n\n\nTo configure GRACC to start at boot, you will need to do the following for the \nosg\n instance:\n\n\nln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload\n\n\n\n\nMultiple instance can be run by editing the instance name above.\n\n\nFinally, these services can be started via the typical system management commands:\n\n\nsystemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg\n\n\n\n\nLog files\n\n\nBy default, log files go into \n/var/log/gracc\n.", 
            "title": "GRACE Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installing-grace", 
            "text": "The GRACE database service consists of:   ElasticSearch as a datastore.  grace-raw-listener : Listens to the raw records from the collector  grace-summary-listener : Listens for and requests summary records.  grace-request-listener : Listens for replay and summarization requests.   See the  agent architecture docs  for more information.  Additionally, for monitoring and visualization, one commonly installs\nthe following external components.   InfluxDB (primarily for monitoring ES performance)  Grafana (visualization)  Kibana (visualization)", 
            "title": "Installing GRACE"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#dependencies", 
            "text": "This document assumes a RHEL7 host with ElasticSearch pre-installed and functioning.  We also assume that the OSG repos and  yum  priorities have been  appropriately configured .", 
            "title": "Dependencies"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#installation", 
            "text": "The relevant components can be pulled in via a meta-RPM:  yum install --enablerepo=osg-development osg-grace", 
            "title": "Installation"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#configuration", 
            "text": "Configuration files are kept in  /etc/gracc/config.d  and  /usr/share/gracc/config.d ; files in this directory are read in lexigraphical order.  The file format is  TOML .  You will likely need to override at least the AMQP connection paramaters (username and password).  Do not edit the default file in  /usr/share/gracc/config.d : these will be overwritten on upgrade.  Instead, start by editing the samples in  /etc .  Most strings will expand the text %I to the \"instance name\".  So, for the  osg  instance, the following,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw   is equivalent to  [AMQP]\nexchange =  gracc.osg.raw \nqueue =  grace.osg.raw   Further, we can have specific instance overrides.  Hence,  [AMQP]\nexchange =  gracc.%I.raw \nqueue =  grace.%I.raw \n\n[AMQP.osg]\nexchange =  gracc.osg-test.raw   is equivalent to:  [AMQP]\nexchange =  gracc.osg-test.raw \nqueue =  gracc.osg.raw", 
            "title": "Configuration"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#running-services", 
            "text": "To configure GRACC to start at boot, you will need to do the following for the  osg  instance:  ln -sf /usr/lib/systemd/system/grace-raw-listener@.service /etc/systemd/system/multi-user.target.wants/grace-raw-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-summary-listener@.service /etc/systemd/system/multi-user.target.wants/grace-summary-listener@osg.service\nln -sf /usr/lib/systemd/system/grace-request-listener@.service /etc/systemd/system/multi-user.target.wants/grace-request-listener@osg.service\nsystemctl daemon-reload  Multiple instance can be run by editing the instance name above.  Finally, these services can be started via the typical system management commands:  systemctl start grace-raw-listener@osg\nsystemctl start grace-summary-listener@osg\nsystemctl start grace-request-listener@osg", 
            "title": "Running services"
        }, 
        {
            "location": "/dev-docs/install-grace-db/#log-files", 
            "text": "By default, log files go into  /var/log/gracc .", 
            "title": "Log files"
        }, 
        {
            "location": "/dev-docs/backups/", 
            "text": "Backup Configuration\n\n\nBackup Sources\n\n\nGRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system.  The \nGRACC Archiver\n listens to the raw RabbitMQ exchanges.  It listens to both the \ngracc.osg.raw\n and the \ngracc.osg-transfer.raw\n exchanges.\n\n\nThe archive agent stores the records into a tar.gz file located in \n/var/lib/graccarchive/sandbox\n.  On new days (or agent crashes) the tar.gz files are atomically copied to \n/var/lib/graccarchive/output\n.  The transfer archiver similarily stores files in \n/var/lib/graccarchive/sandbox-transfer\n and \n/var/lib/graccarchive/output-transfer\n.\n\n\nThe archive agent is configured in \n/etc/graccarchive/conf\n.  It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time.\n\n\nBackup Location\n\n\nThe backups are copied to FNAL by the \ngracc-backup\n tool.  This uses SystemD timer and service files to periodically copy the output \n.tar.gz\n files to FNAL.  The final destination (gsiftp) of the files is configured in the *.service files with the tool\n\n\nRestore Operation\n\n\nThe restore operation uses the \ngraccunarchiver\n tool distributed with the GRACC Archiver agent.  The workflow of a restore is:\n\n\n\n\nCopy the backup file from the backup location.  You will likely need to use \nglobus-url-copy\n in order to copy the files back from the backup location.\n\n\nRun the \ngraccunarchiver\n tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters.\ngraccunarchiver \nrabbitmq_url\n gracc.osg.raw gracc-2017-04-04.tar.gz", 
            "title": "GRACE Backup Documentation"
        }, 
        {
            "location": "/dev-docs/backups/#backup-configuration", 
            "text": "", 
            "title": "Backup Configuration"
        }, 
        {
            "location": "/dev-docs/backups/#backup-sources", 
            "text": "GRACC backup sources come from listening and duplicating all raw records sent to the system through the RabbitMQ system.  The  GRACC Archiver  listens to the raw RabbitMQ exchanges.  It listens to both the  gracc.osg.raw  and the  gracc.osg-transfer.raw  exchanges.  The archive agent stores the records into a tar.gz file located in  /var/lib/graccarchive/sandbox .  On new days (or agent crashes) the tar.gz files are atomically copied to  /var/lib/graccarchive/output .  The transfer archiver similarily stores files in  /var/lib/graccarchive/sandbox-transfer  and  /var/lib/graccarchive/output-transfer .  The archive agent is configured in  /etc/graccarchive/conf .  It uses systemd template units to run both the raw jobs and raw transfer archivers at the same time.", 
            "title": "Backup Sources"
        }, 
        {
            "location": "/dev-docs/backups/#backup-location", 
            "text": "The backups are copied to FNAL by the  gracc-backup  tool.  This uses SystemD timer and service files to periodically copy the output  .tar.gz  files to FNAL.  The final destination (gsiftp) of the files is configured in the *.service files with the tool", 
            "title": "Backup Location"
        }, 
        {
            "location": "/dev-docs/backups/#restore-operation", 
            "text": "The restore operation uses the  graccunarchiver  tool distributed with the GRACC Archiver agent.  The workflow of a restore is:   Copy the backup file from the backup location.  You will likely need to use  globus-url-copy  in order to copy the files back from the backup location.  Run the  graccunarchiver  tool from the GRACC Archiver on the compressed .tar.gz file, with command line arguments for the RabbitMQ parameters. graccunarchiver  rabbitmq_url  gracc.osg.raw gracc-2017-04-04.tar.gz", 
            "title": "Restore Operation"
        }
    ]
}